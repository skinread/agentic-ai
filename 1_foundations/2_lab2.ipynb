{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 2: Week 1\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Important point - please read</h2>\n",
    "            <span style=\"color:#00bfff;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-svcac\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you approach resolving a moral dilemma involving a self-driving car faced with an unavoidable accident, where it must choose between the lesser harm to its occupants or pedestrians, considering various ethical frameworks such as utilitarianism, deontology, and virtue ethics?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Resolving a moral dilemma involving a self-driving car in an unavoidable accident requires careful consideration of various ethical frameworks. Here’s an approach based on utilitarianism, deontology, and virtue ethics:\n",
       "\n",
       "### 1. **Utilitarianism**\n",
       "Utilitarianism focuses on the greatest good for the greatest number. In the context of a self-driving car:\n",
       "\n",
       "- **Analysis**: Assess the potential outcomes and their impacts on all parties involved—occupants of the car and pedestrians. The car should be programmed to minimize overall harm.\n",
       "- **Decision**: If it's determined that sacrificing the occupants results in a lesser total harm (for instance, saving more lives) then the car would ideally prioritize avoiding greater loss of life.\n",
       "- **Considerations**: This approach might lead to challenging questions about how to quantify lives or lives in different contexts, which is a significant limitation of utilitarianism.\n",
       "\n",
       "### 2. **Deontology**\n",
       "Deontological ethics emphasizes rules and duties over consequences. In this view:\n",
       "\n",
       "- **Analysis**: Identify moral rules that govern the situation. This includes the sanctity of life, the duty to protect passengers, and the obligation to avoid harm to others.\n",
       "- **Decision**: The car could be programmed to prioritize not harming pedestrians as a moral duty, although this could lead to an ethical conflict if it means endangering the occupants.\n",
       "- **Considerations**: This framework could argue that the car should not intentionally harm any group, even if it leads to a worse overall outcome. It raises questions about pre-programmed moral rules and their implications.\n",
       "\n",
       "### 3. **Virtue Ethics**\n",
       "Virtue ethics focuses on character and the moral virtues that a good person would exemplify in decision-making:\n",
       "\n",
       "- **Analysis**: Consider what a virtuous person would do in this situation—traits like courage, compassion, and fairness might guide the decision.\n",
       "- **Decision**: The self-driving car could be programmed in a way that embodies virtues by taking actions that reflect a deeper understanding of human dignity and respect for life, seeking to minimize harm while recognizing the value of both passengers and pedestrians.\n",
       "- **Considerations**: This framework emphasizes moral character and intentions rather than just rules or outcomes, which could lead to more nuanced programming and ethical discussions.\n",
       "\n",
       "### Integration\n",
       "In practice, designing the ethical framework of a self-driving car might involve:\n",
       "\n",
       "- **Balancing Frameworks**: A hybrid approach that incorporates elements from all three frameworks could be considered. For example, using a utilitarian basis grounded in deontological principles to avoid violating moral rules could create a more balanced system.\n",
       "- **Stakeholder Involvement**: Engaging stakeholders (including ethicists, engineers, consumers, and policymakers) to reach a consensus on how these ethical principles should be implemented helps address the diversity of moral beliefs in society.\n",
       "- **Transparency**: Clearly communicating how ethical decisions are made programmatically can help users understand the vehicle's responses in crisis situations.\n",
       "\n",
       "Ultimately, creating a self-driving car capable of ethical decision-making involves grappling with complex ethical challenges and balancing competing moral frameworks, ensuring decisions are made in alignment with societal values and expectations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Approaching the Self-Driving Car Moral Dilemma\n",
       "\n",
       "This classic \"trolley problem\" for autonomous vehicles requires balancing multiple ethical frameworks:\n",
       "\n",
       "## Utilitarian Perspective\n",
       "I would consider minimizing overall harm by assessing:\n",
       "- Number of potential casualties in each scenario\n",
       "- Probability of survival in different impact scenarios\n",
       "- Age and vulnerability factors (though this introduces bias concerns)\n",
       "\n",
       "## Deontological Perspective\n",
       "I would examine:\n",
       "- The duty not to use persons as mere means\n",
       "- Whether programming a car to sacrifice its passengers violates an implicit contract with consumers\n",
       "- Whether the car's duty is primarily to protect its occupants or the public\n",
       "\n",
       "## Virtue Ethics Approach\n",
       "I would consider what decision reflects:\n",
       "- Fairness and impartiality in valuing all human life\n",
       "- Prudence in designing systems that minimize such dilemmas occurring\n",
       "- Courage in facing difficult ethical trade-offs\n",
       "\n",
       "## Practical Implementation\n",
       "A comprehensive approach might involve:\n",
       "- Prioritizing accident prevention over accident optimization\n",
       "- Programming primarily for safety, not moral calculus\n",
       "- Transparency with users about decision frameworks\n",
       "- Involving diverse stakeholders in determining standards\n",
       "\n",
       "Rather than a simple algorithm, the solution likely requires a multi-layered approach balancing these considerations with practical safety engineering."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Resolving a moral dilemma for a self-driving car in an unavoidable accident is one of the most challenging ethical problems facing AI and autonomous systems. There's no single, universally agreed-upon solution, as different ethical frameworks lead to different conclusions, and practical implementation adds further complexity.\n",
       "\n",
       "Here's how one might approach it, considering the frameworks:\n",
       "\n",
       "## The Dilemma: Unavoidable Accident - Occupants vs. Pedestrians\n",
       "\n",
       "**The Scenario:** A self-driving car is traveling down a street. Suddenly, a child darts into the road. The car's sensors detect the child, and its AI determines that a collision is unavoidable.\n",
       "*   **Option A:** Swerve into a wall, likely killing or severely injuring the occupants, but saving the child.\n",
       "*   **Option B:** Continue straight, hitting the child, likely killing the child but saving the occupants.\n",
       "*   **Option C:** Swerve into an oncoming lane, risking a multi-car pileup, potentially causing more casualties than either A or B. (Let's focus on A vs. B for simplicity, the core occupant vs. pedestrian dilemma).\n",
       "\n",
       "## Ethical Frameworks Applied:\n",
       "\n",
       "### 1. Utilitarianism (Consequentialism)\n",
       "\n",
       "*   **Core Principle:** The best action is the one that maximizes overall good (utility) and minimizes overall harm. It focuses on the consequences of the action.\n",
       "*   **Application:**\n",
       "    *   **The \"Numbers Game\":** A purely utilitarian approach would calculate the number of lives saved vs. lost. If hitting the child means one fatality and swerving into the wall means two occupant fatalities, the utilitarian choice would be to hit the child (one fatality is less than two). Conversely, if hitting the child means two fatalities (e.g., a child and a parent pushing a stroller) and swerving into the wall means one occupant fatality, the utilitarian choice would be to swerve.\n",
       "    *   **Quality of Life/Long-term Impact:** Some utilitarian models might attempt to factor in the quality of life (e.g., a healthy young person vs. an elderly person with a terminal illness, or severe injury vs. death), but this becomes incredibly complex and morally contentious.\n",
       "    *   **Societal Utility:** Beyond immediate lives, a utilitarian might consider the broader impact on public trust in autonomous vehicles. If cars are programmed to sacrifice their occupants, people might be less willing to buy them, which could ironically lead to more accidents overall (if human-driven cars are less safe than AVs).\n",
       "*   **Challenges:**\n",
       "    *   **Quantifying Life:** How do you assign a value to a life? Is one life inherently \"worth\" more than another?\n",
       "    *   **Predictive Accuracy:** The car's AI might not have enough information to accurately predict the *exact* outcome of each choice (e.g., will hitting the wall *definitely* kill the occupants, or just injure them severely?).\n",
       "    *   **The \"Trolley Problem\" Criticism:** Utilitarianism can lead to outcomes that feel intuitively unjust, especially if it involves actively choosing to harm someone for the greater good (e.g., sacrificing an innocent individual).\n",
       "\n",
       "### 2. Deontology (Duty-Based Ethics)\n",
       "\n",
       "*   **Core Principle:** Certain actions are inherently right or wrong, regardless of their consequences. Morality is based on duties, rules, and moral absolutes (e.g., \"Do not kill,\" \"Do not use a person as a means to an end\"). Immanuel Kant's Categorical Imperative is a key concept.\n",
       "*   **Application:**\n",
       "    *   **Protecting Occupants (Contractual Duty):** One deontological perspective argues that the car (or its manufacturer) has a primary duty to protect its occupants, as they are the direct customers who have entered into a \"contract\" for safe transport. Intentionally swerving to harm them would violate this duty.\n",
       "    *   **Do No Harm (Non-Maleficence):** Another deontological rule is \"do no harm.\" This creates a conflict:\n",
       "        *   Intentionally swerving to kill occupants to save pedestrians is an active act of harm.\n",
       "        *   Continuing straight and hitting a pedestrian is also an act of harm, though it might be framed as a failure to avoid harm rather than an intentional choice to *cause* harm to the pedestrian.\n",
       "    *   **Means to an End:** Deontology typically prohibits using a person as a mere means to an end. Sacrificing occupants to save pedestrians could be seen as using the occupants as a means to save others.\n",
       "    *   **Rule-Based Logic:** A deontological approach might lead to a simple rule: \"The car will prioritize avoiding *causing* new harm over merely mitigating existing harm.\" Or, \"The car will minimize its own active intervention in a way that causes new harm.\"\n",
       "*   **Challenges:**\n",
       "    *   **Conflicting Duties:** This framework struggles when duties conflict. The duty to protect occupants conflicts with the duty to avoid harming pedestrians. Which duty takes precedence?\n",
       "    *   **Absolutes vs. Reality:** In real-world unavoidable accidents, *some* harm is inevitable. Strict adherence to \"do no harm\" may not be possible.\n",
       "    *   **Lack of Flexibility:** A rigid rule-based system might not adapt to nuanced situations.\n",
       "\n",
       "### 3. Virtue Ethics\n",
       "\n",
       "*   **Core Principle:** Focuses on the character of the moral agent rather than the specific actions or consequences. It asks: \"What would a virtuous person do?\" It emphasizes virtues like wisdom, courage, justice, compassion, and prudence.\n",
       "*   **Application:**\n",
       "    *   **Less Direct for Machines:** A machine doesn't *have* virtues in the human sense. This framework is less about the car's \"decision\" and more about the virtues that the *designers, programmers, and policymakers* should embody when creating the car's ethical guidelines.\n",
       "    *   **Virtues for Designers:**\n",
       "        *   **Wisdom/Prudence:** Designing a system that considers all foreseeable scenarios, understands limitations, and seeks the most reasonable solution.\n",
       "        *   **Justice:** Ensuring the decision-making process is fair and doesn't discriminate based on wealth, social status, or other arbitrary factors.\n",
       "        *   **Compassion:** Programming the car to minimize suffering and loss of life overall, showing care for all individuals involved.\n",
       "        *   **Courage:** Making difficult decisions in design, even if they are unpopular or controversial, for the greater good.\n",
       "    *   **Societal Character:** Virtue ethics might prompt us to ask what kind of society we want to foster with these vehicles. Do we want a society where machines are programmed to sacrifice individuals? Or one where they prioritize the safety of those who paid for the service?\n",
       "*   **Challenges:**\n",
       "    *   **Operationalizing Virtues:** How do you translate abstract virtues into concrete programming rules for a machine?\n",
       "    *   **Subjectivity:** What one person considers virtuous, another might not.\n",
       "    *   **Does Not Provide a Clear \"Decision Rule\":** Virtue ethics offers guiding principles for the *designers*, but doesn't directly tell the car what to do in the moment of crisis.\n",
       "\n",
       "## Towards a Resolution (Practical & Hybrid Approaches)\n",
       "\n",
       "Given the complexities, no single framework provides a complete or universally acceptable answer. A practical approach would likely involve a hybrid of these, alongside legal and societal considerations:\n",
       "\n",
       "1.  **Prioritizing Minimizing Harm (Utilitarian Influence):** In most public discussions, the idea of minimizing overall casualties holds significant weight. While not always easy, a primary goal would be to reduce the total number of fatalities or severe injuries.\n",
       "\n",
       "2.  **No Intentional Harming (Deontological Influence):** Many argue that a machine should *never* be programmed to intentionally choose to kill or seriously injure specific individuals. Instead, its programming should focus on *risk mitigation* and *accident avoidance*. If an accident is unavoidable, it should mitigate the impact in the least harmful way *without making an active \"choice\" to sacrifice someone*. This is a subtle but important distinction. For example, if continuing straight leads to an unavoidable impact with a pedestrian, it's not the car \"choosing\" to kill the pedestrian; it's the car failing to avoid an *unavoidable* impact. Swerving to kill occupants, however, is an active choice to cause harm.\n",
       "\n",
       "3.  **Default to Occupant Safety (Deontological/Contractual Influence):** A common view is that the primary duty of a vehicle (and its manufacturer) is to protect its occupants. This aligns with legal precedent regarding product liability and consumer expectations. If this means prioritizing the occupants' safety over pedestrians *in an unavoidable collision*, that might be the default. However, this is highly controversial and often seen as morally problematic, leading to the \"murderous trolley car\" problem.\n",
       "\n",
       "4.  **Avoid Making the \"Choice\":** Instead of programming cars to \"choose\" who dies, the focus should be on:\n",
       "    *   **Accident Prevention:** The vast majority of resources should go into making AVs *so safe* that these dilemmas are vanishingly rare.\n",
       "    *   **Damage Mitigation:** If an accident is unavoidable, the car's priority should be to minimize damage to all parties, potentially by braking maximally, bracing, or deploying airbags, rather than steering into another object/person.\n",
       "    *   **Deterministic vs. Stochastic:** The car's behavior should be predictable and consistent, rather than making complex, real-time moral calculations that humans struggle with.\n",
       "\n",
       "5.  **Transparency and Public Acceptance:** Whatever ethical rules are adopted, they must be transparent and, ideally, subject to broad public discussion and consensus. If people don't trust the ethical programming of self-driving cars, adoption will suffer.\n",
       "\n",
       "6.  **Legal and Liability Frameworks:** These ethical dilemmas need to be addressed in law. Who is liable when a self-driving car makes a \"moral\" choice that results in harm? The manufacturer, the owner, the programmer?\n",
       "\n",
       "Ultimately, the goal is not to program a car to be a moral philosopher, but to embed a set of defensible, consistent rules that lead to outcomes that society deems acceptable, while acknowledging the inherent tragedy of unavoidable accidents. Many experts advocate for programming that prioritizes **minimizing overall harm** while **avoiding intentional sacrifice**, with a strong emphasis on **accident prevention** as the ultimate ethical goal."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash-preview-05-20\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#00bfff;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Exercise</h2>\n",
    "            <span style=\"color:#00bfff;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/commercial.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            and common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
