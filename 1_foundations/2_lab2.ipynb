{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 2: Week 1\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Important point - please read</h2>\n",
    "            <span style=\"color:#00bfff;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-svcac\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the potential ethical implications of using advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, and how might these implications vary across different cultural contexts?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The use of advanced AI systems in decision-making processes related to public health, especially in marginalized communities, entails several ethical implications that can vary significantly across different cultural contexts. Here are some key considerations:\n",
       "\n",
       "### 1. **Bias and Discrimination**\n",
       "- **Implication**: AI systems are often trained on historical data that may reflect existing biases, potentially leading to discriminatory outcomes against marginalized communities.\n",
       "- **Cultural Variation**: In some cultures, historical injustices may compound the effect of AI biases, making the consequences more pronounced. For instance, in a society with a history of systemic racism, biased AI algorithms could exacerbate health disparities.\n",
       "\n",
       "### 2. **Informed Consent and Autonomy**\n",
       "- **Implication**: The deployment of AI in public health decision-making can overlook the importance of informed consent, especially if marginalized communities do not fully understand the technology being used.\n",
       "- **Cultural Variation**: In cultures with different values regarding autonomy and authority, the concept of informed consent might be interpreted differently. Populations with communal decision-making traditions may view collective consent as more important than individual consent.\n",
       "\n",
       "### 3. **Transparency and Explainability**\n",
       "- **Implication**: AI systems can be \"black boxes,\" making it challenging for communities to understand how decisions affecting their health are made.\n",
       "- **Cultural Variation**: In cultures that emphasize transparency and accountability in governance, a lack of explainability could lead to mistrust towards health authorities and the technology itself, whereas in other cultures, reliance on expert opinion may reduce the demand for transparency.\n",
       "\n",
       "### 4. **Access and Equity**\n",
       "- **Implication**: Advanced AI tools might not be equally accessible to all communities, potentially widening the gap in health outcomes.\n",
       "- **Cultural Variation**: In regions with significant income inequality or disparities in technological infrastructure, marginalized communities may benefit less from AI advancements, while wealthier areas may utilize these technologies more effectively.\n",
       "\n",
       "### 5. **Data Privacy and Surveillance**\n",
       "- **Implication**: The use of AI in public health can lead to increased surveillance, raising concerns about individual privacy, especially in communities less equipped to advocate for their rights.\n",
       "- **Cultural Variation**: Societies with strong privacy laws may push back against data misuse, while in collectivist cultures, data may be viewed as a community resource, leading to conflicts over privacy versus communal benefit.\n",
       "\n",
       "### 6. **Trust in Institutions**\n",
       "- **Implication**: Trust in health institutions can be jeopardized if communities perceive AI as undermining their voices in public health decisions.\n",
       "- **Cultural Variation**: Cultures with a higher level of trust in medical or governmental authority may accept AI-driven recommendations more readily than those with historical skepticism towards these institutions.\n",
       "\n",
       "### 7. **Resilience and Capacity Building**\n",
       "- **Implication**: Relying solely on AI systems can undermine local capacities for resilience and self-determination in health interventions.\n",
       "- **Cultural Variation**: Different cultural attitudes towards technology and tradition may affect how communities rate their ability to engage with, adopt, or resist AI-driven initiatives.\n",
       "\n",
       "### 8. **Ethical Considerations in AI Development**\n",
       "- **Implication**: The ethical frameworks guiding AI design can vary significantly, potentially leading to prototypes that do not align with the values or needs of marginalized communities.\n",
       "- **Cultural Variation**: In some cultures, indigenous knowledge systems and traditional practices regarding health may conflict with the data-driven approaches of AI, raising questions about whose values and knowledge are prioritized.\n",
       "\n",
       "### Conclusion\n",
       "Addressing these ethical implications requires a culturally aware approach that involves careful consideration of local contexts, active engagement with communities, and the formulation of guidelines that emphasize equity, justice, and respect for diverse values. Policymakers and AI developers should prioritize inclusivity and participatory methods to ensure that AI tools in public health contribute positively to marginalized communities, rather than perpetuating or exacerbating existing inequalities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Ethical Implications of AI in Public Health Decision-Making\n",
       "\n",
       "## Core Ethical Considerations\n",
       "\n",
       "**Algorithmic Bias and Health Disparities**\n",
       "AI systems trained on historically biased data may perpetuate or amplify existing health inequities in marginalized communities. This could manifest in resource misallocation, misdiagnosis, or inappropriate treatment recommendations.\n",
       "\n",
       "**Autonomy and Informed Consent**\n",
       "Questions arise about who controls health decisions when AI systems are involved, particularly in communities with limited technological literacy or where cultural concepts of autonomy differ from Western frameworks.\n",
       "\n",
       "**Transparency and Explainability**\n",
       "\"Black box\" AI systems may make critical health decisions without clear explanations, limiting accountability and trust, especially in communities with historical reasons to distrust healthcare authorities.\n",
       "\n",
       "## Cultural Context Variations\n",
       "\n",
       "**Collectivist vs. Individualist Societies**\n",
       "- In collectivist cultures, AI systems designed with Western individualistic assumptions about privacy and decision-making may conflict with community-centered approaches to health.\n",
       "- Different cultures may have varying comfort levels with technological intervention in health decisions.\n",
       "\n",
       "**Religious and Traditional Frameworks**\n",
       "Health decisions in many communities are guided by religious or traditional practices that AI systems may not adequately account for or may actively contradict.\n",
       "\n",
       "**Power Dynamics**\n",
       "In postcolonial contexts or regions with histories of medical exploitation, AI systems deployed by external entities may be perceived as continuing patterns of extraction and control rather than empowerment.\n",
       "\n",
       "## Mitigation Approaches\n",
       "\n",
       "Effective approaches would include community co-design of AI systems, cultural competency requirements in AI development, and governance frameworks that prioritize local autonomy while benefiting from AI capabilities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The use of advanced AI systems in public health decision-making holds immense promise for improving efficiency, diagnostics, and resource allocation. However, deploying these systems, especially in marginalized communities, introduces complex ethical challenges that are further complicated by varying cultural contexts.\n",
       "\n",
       "Here are the potential ethical implications:\n",
       "\n",
       "### Ethical Implications for Public Health Decision-Making in Marginalized Communities:\n",
       "\n",
       "1.  **Algorithmic Bias and Amplification of Health Disparities:**\n",
       "    *   **Data Bias:** AI systems learn from historical data. If this data disproportionately represents certain groups, contains historical biases (e.g., medical racism, under-diagnosis of certain conditions in specific ethnic groups), or has gaps for marginalized communities, the AI will learn and perpetuate these biases. For instance, diagnostic AI trained primarily on data from lighter skin tones might perform poorly on darker skin.\n",
       "    *   **Outcome Bias:** AI models can reflect and even exacerbate existing socioeconomic and health inequities. An AI recommending resource allocation might, based on historical patterns, de-prioritize communities that have been historically underserved, leading to reduced access to care or lower quality interventions.\n",
       "    *   **Proxy Discrimination:** AI might use proxies (e.g., zip codes, education levels) that correlate with race, income, or other protected characteristics, leading to indirect discrimination, even if the system isn't explicitly programmed to do so.\n",
       "\n",
       "2.  **Privacy and Surveillance:**\n",
       "    *   **Excessive Data Collection:** AI systems require vast amounts of data, including sensitive health, genetic, and even behavioral information. Marginalized communities, often under greater scrutiny or with less digital literacy, might be disproportionately subject to data collection without full informed consent or understanding of its implications.\n",
       "    *   **Re-identification Risk:** Even \"anonymized\" data can sometimes be re-identified, especially with advanced AI techniques, posing risks to individuals and groups.\n",
       "    *   **Dual-Use Potential:** Public health data collected by AI could be misused for surveillance, immigration enforcement, or criminal justice purposes, eroding trust, particularly in communities with historical mistrust of authorities.\n",
       "\n",
       "3.  **Autonomy, Consent, and Paternalism:**\n",
       "    *   **Lack of Informed Consent:** Obtaining truly informed consent for complex AI interventions can be challenging. For marginalized communities, this is compounded by language barriers, lower literacy rates, power imbalances, and historical experiences of exploitation (e.g., Tuskegee Syphilis Study).\n",
       "    *   **Coercion/Pressure:** If AI systems become the primary decision-makers or gatekeepers for essential services, individuals might feel pressured to comply with AI recommendations even if they don't fully understand or agree, eroding their autonomy.\n",
       "    *   **Digital Divide:** Lack of access to technology, reliable internet, or digital literacy can exclude marginalized individuals from participating in or benefiting from AI-driven public health initiatives, further disempowering them.\n",
       "\n",
       "4.  **Accountability and Transparency (The \"Black Box\" Problem):**\n",
       "    *   **Lack of Explainability:** Many advanced AI systems (deep learning) are \"black boxes,\" meaning their decision-making processes are opaque. When an AI system makes a critical public health decision (e.g., recommending a lockdown, triaging patients), it's hard to understand *why* it made that choice.\n",
       "    *   **Diffusion of Responsibility:** If an AI makes a wrong or harmful decision, who is accountable? The developer? The public health official who deployed it? This becomes even more complex when the impact falls disproportionately on vulnerable groups.\n",
       "    *   **Erosion of Trust:** If public health decisions are made by inscrutable algorithms, it can foster mistrust, especially if those decisions appear to perpetuate existing injustices or discriminate against certain groups.\n",
       "\n",
       "5.  **Equity and Access:**\n",
       "    *   **Exacerbating Resource Inequality:** If AI systems are costly to develop and deploy, they might be first adopted by well-resourced health systems, potentially widening the gap between areas with advanced AI tools and those without.\n",
       "    *   **Focus on Quantifiable Metrics:** AI systems tend to optimize for quantifiable metrics. This might lead to overlooking crucial qualitative factors, social determinants of health, or community-specific needs that are harder to measure but vital for marginalized communities.\n",
       "\n",
       "### Variation Across Different Cultural Contexts:\n",
       "\n",
       "Cultural context profoundly shapes how these ethical implications manifest and how acceptable AI-driven public health interventions are perceived.\n",
       "\n",
       "1.  **Data Privacy & Ownership:**\n",
       "    *   **Individualistic vs. Collectivistic Cultures:** Western individualistic cultures often emphasize personal data privacy and consent. In many collectivistic cultures, data might be viewed more communally, and consent might involve family elders or community leaders rather than just the individual. However, this doesn't necessarily mean less concern about privacy; it's just framed differently.\n",
       "    *   **Trust in Institutions:** Cultures vary widely in their trust in government, medical institutions, and technology companies. Low trust (e.g., due to historical colonial abuses or authoritarian regimes) will make AI adoption in public health far more challenging, regardless of the AI's technical merit.\n",
       "    *   **Indigenous Data Sovereignty:** Many Indigenous communities assert inherent rights to control their data, including health data, reflecting unique cultural understandings of information, knowledge, and self-determination.\n",
       "\n",
       "2.  **Concept of Health & Well-being:**\n",
       "    *   **Biomedical vs. Holistic Views:** Many cultures have a holistic view of health, integrating physical, mental, spiritual, and community well-being. AI optimized for purely biomedical outcomes might miss or even contradict these broader cultural understandings, leading to ineffective or culturally inappropriate interventions.\n",
       "    *   **Traditional Medicine:** In cultures where traditional or indigenous medicine plays a significant role, AI-driven public health approaches might clash with or be seen as dismissive of established healing practices.\n",
       "\n",
       "3.  **Decision-Making and Autonomy:**\n",
       "    *   **Family/Community Involvement:** In many cultures, health decisions are not purely individual but involve family members, elders, or community leaders. AI systems designed for individual-centric decision-making might not integrate well into these processes.\n",
       "    *   **Fatalism/Determinism:** Some cultural beliefs might influence attitudes towards health interventions, potentially affecting willingness to engage with AI-driven preventative or diagnostic tools.\n",
       "\n",
       "4.  **Communication and Language:**\n",
       "    *   **Linguistic Diversity:** Public health interventions must be linguistically appropriate. AI systems need to be trained on diverse linguistic datasets, and their outputs must be comprehensible and culturally resonant.\n",
       "    *   **Non-Verbal Cues:** AI relies heavily on structured data. In many cultures, non-verbal cues, storytelling, and nuanced communication are crucial for understanding health concerns and building rapport, which AI currently struggles to interpret.\n",
       "\n",
       "5.  **Spirituality and Belief Systems:**\n",
       "    *   **Religious/Spiritual Influence:** Religious or spiritual beliefs can profoundly influence health behaviors, treatment choices, and attitudes towards technology. AI interventions that do not respectfully acknowledge or integrate these beliefs may face strong resistance.\n",
       "\n",
       "In conclusion, while advanced AI systems offer significant potential for public health, their deployment, especially in marginalized communities and across diverse cultural contexts, requires profound ethical consideration. A human-centered, culturally sensitive, and community-engaged approach is crucial to ensure these systems genuinely improve health equity rather than exacerbating existing disparities or eroding fundamental rights and values. This necessitates diverse development teams, robust regulatory frameworks, continuous auditing for bias, and meaningful participation from the communities served."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash-preview-05-20\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#00bfff;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 74701a8c35f6: 100% ▕██████████████████▏ 1.3 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 4f659a1e86d7: 100% ▕██████████████████▏  485 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises several potential ethical implications. Here are some possible considerations:\n",
       "\n",
       "1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory practices if they are trained on biased data or designed with a particular worldview. For example, an AI system that categorizes individuals based on their socioeconomic status or ethnicity may unfairly penalize or benefit marginalized communities.\n",
       "2. **Lack of transparency and accountability**: Complex AI decision-making processes can be opaque, making it difficult to understand how decisions were made. This lack of transparency can erode trust in the system and create opportunities for abuse.\n",
       "3. **Extrication bias**: AI systems may be trained on datasets that reflect existing power dynamics, which can lead to \"inferred\" or unintentionally biased decision-making processes.\n",
       "4. **Limited contextual understanding**: AI systems may not fully understand the nuances of human behavior, culture, and contexts, leading to decisions that are not tailored to specific populations.\n",
       "\n",
       "Ethical implications across different cultural contexts:\n",
       "\n",
       "1. **Diversity and inclusion**:\n",
       "In some cultures, AI systems may be seen as more trustworthy or capable than those developed in other countries due to factors like language proficiency or cultural norms.\n",
       "2. **Contextual understanding**: The extent of contextual awareness varies across cultures, and AI systems designed for one cultural context may not perform well in others.\n",
       "3. **Power dynamics**: In some cultures, there may be existing power imbalances that influence how AI systems are developed, deployed, or controlled.\n",
       "4. **Linguistic and social nuances**: Cultural differences in communication styles, idioms, and social norms can affect the way people understand and interpret AI-generated outputs.\n",
       "\n",
       "Examples of cultural variations:\n",
       "\n",
       "1. In some South Asian cultures, there may be a tendency to use AI-powered systems to filter out perceived \"undesirables,\" such as individuals with disabilities or marginalized groups.\n",
       "2. In some Indigenous communities, AI systems may be used to track land use patterns and monitor environmental changes, but these efforts may be framed in ways that prioritize European colonialism or resource extraction.\n",
       "3. In Arab cultures, social media is highly influential in shaping public opinion, which can be leveraged by policymakers to influence AI decision-making processes.\n",
       "\n",
       "Varying implications across different cultural contexts:\n",
       "\n",
       "1. **More severe biases**: AI system design and training may exacerbate existing biases in more culturally diverse societies, perpetuating systemic inequalities.\n",
       "2. **More complex solutions**: In contrast, cultural nuances might lead to more nuanced and context-specific AI solutions that account for power dynamics, language use, and social norms.\n",
       "\n",
       "To mitigate these concerns, it is essential to:\n",
       "\n",
       "1. **Conduct inclusive research and testing** across various cultural contexts to ensure AI systems are developed with a deep understanding of diverse experiences.\n",
       "2. **Engage in iterative, community-driven processes** where stakeholders collaborate on designing, deploying, and monitoring AI systems that respect cultural differences and values.\n",
       "3. **Implement robust audit trails**, data protection policies, and transparent reporting mechanisms to maintain trust and accountability.\n",
       "\n",
       "Ultimately, effective AI governance requires a multi-faceted approach that balances technological innovation with social responsibility, inclusivity, and responsiveness to diverse contexts and cultures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2:1b\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'claude-3-7-sonnet-latest', 'llama3.2:1b', 'gemini-2.5-flash-preview-05-20', 'llama3.2:1b']\n",
      "['The use of advanced AI systems in decision-making processes related to public health, especially in marginalized communities, entails several ethical implications that can vary significantly across different cultural contexts. Here are some key considerations:\\n\\n### 1. **Bias and Discrimination**\\n- **Implication**: AI systems are often trained on historical data that may reflect existing biases, potentially leading to discriminatory outcomes against marginalized communities.\\n- **Cultural Variation**: In some cultures, historical injustices may compound the effect of AI biases, making the consequences more pronounced. For instance, in a society with a history of systemic racism, biased AI algorithms could exacerbate health disparities.\\n\\n### 2. **Informed Consent and Autonomy**\\n- **Implication**: The deployment of AI in public health decision-making can overlook the importance of informed consent, especially if marginalized communities do not fully understand the technology being used.\\n- **Cultural Variation**: In cultures with different values regarding autonomy and authority, the concept of informed consent might be interpreted differently. Populations with communal decision-making traditions may view collective consent as more important than individual consent.\\n\\n### 3. **Transparency and Explainability**\\n- **Implication**: AI systems can be \"black boxes,\" making it challenging for communities to understand how decisions affecting their health are made.\\n- **Cultural Variation**: In cultures that emphasize transparency and accountability in governance, a lack of explainability could lead to mistrust towards health authorities and the technology itself, whereas in other cultures, reliance on expert opinion may reduce the demand for transparency.\\n\\n### 4. **Access and Equity**\\n- **Implication**: Advanced AI tools might not be equally accessible to all communities, potentially widening the gap in health outcomes.\\n- **Cultural Variation**: In regions with significant income inequality or disparities in technological infrastructure, marginalized communities may benefit less from AI advancements, while wealthier areas may utilize these technologies more effectively.\\n\\n### 5. **Data Privacy and Surveillance**\\n- **Implication**: The use of AI in public health can lead to increased surveillance, raising concerns about individual privacy, especially in communities less equipped to advocate for their rights.\\n- **Cultural Variation**: Societies with strong privacy laws may push back against data misuse, while in collectivist cultures, data may be viewed as a community resource, leading to conflicts over privacy versus communal benefit.\\n\\n### 6. **Trust in Institutions**\\n- **Implication**: Trust in health institutions can be jeopardized if communities perceive AI as undermining their voices in public health decisions.\\n- **Cultural Variation**: Cultures with a higher level of trust in medical or governmental authority may accept AI-driven recommendations more readily than those with historical skepticism towards these institutions.\\n\\n### 7. **Resilience and Capacity Building**\\n- **Implication**: Relying solely on AI systems can undermine local capacities for resilience and self-determination in health interventions.\\n- **Cultural Variation**: Different cultural attitudes towards technology and tradition may affect how communities rate their ability to engage with, adopt, or resist AI-driven initiatives.\\n\\n### 8. **Ethical Considerations in AI Development**\\n- **Implication**: The ethical frameworks guiding AI design can vary significantly, potentially leading to prototypes that do not align with the values or needs of marginalized communities.\\n- **Cultural Variation**: In some cultures, indigenous knowledge systems and traditional practices regarding health may conflict with the data-driven approaches of AI, raising questions about whose values and knowledge are prioritized.\\n\\n### Conclusion\\nAddressing these ethical implications requires a culturally aware approach that involves careful consideration of local contexts, active engagement with communities, and the formulation of guidelines that emphasize equity, justice, and respect for diverse values. Policymakers and AI developers should prioritize inclusivity and participatory methods to ensure that AI tools in public health contribute positively to marginalized communities, rather than perpetuating or exacerbating existing inequalities.', '# Ethical Implications of AI in Public Health Decision-Making\\n\\n## Core Ethical Considerations\\n\\n**Algorithmic Bias and Health Disparities**\\nAI systems trained on historically biased data may perpetuate or amplify existing health inequities in marginalized communities. This could manifest in resource misallocation, misdiagnosis, or inappropriate treatment recommendations.\\n\\n**Autonomy and Informed Consent**\\nQuestions arise about who controls health decisions when AI systems are involved, particularly in communities with limited technological literacy or where cultural concepts of autonomy differ from Western frameworks.\\n\\n**Transparency and Explainability**\\n\"Black box\" AI systems may make critical health decisions without clear explanations, limiting accountability and trust, especially in communities with historical reasons to distrust healthcare authorities.\\n\\n## Cultural Context Variations\\n\\n**Collectivist vs. Individualist Societies**\\n- In collectivist cultures, AI systems designed with Western individualistic assumptions about privacy and decision-making may conflict with community-centered approaches to health.\\n- Different cultures may have varying comfort levels with technological intervention in health decisions.\\n\\n**Religious and Traditional Frameworks**\\nHealth decisions in many communities are guided by religious or traditional practices that AI systems may not adequately account for or may actively contradict.\\n\\n**Power Dynamics**\\nIn postcolonial contexts or regions with histories of medical exploitation, AI systems deployed by external entities may be perceived as continuing patterns of extraction and control rather than empowerment.\\n\\n## Mitigation Approaches\\n\\nEffective approaches would include community co-design of AI systems, cultural competency requirements in AI development, and governance frameworks that prioritize local autonomy while benefiting from AI capabilities.', 'The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises significant ethical concerns. Here are some potential implications and considerations that may vary across different cultural contexts:\\n\\n**Ethical concerns:**\\n\\n1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory patterns if they are trained on biased data or designed with a discriminatory goal. This can lead to unequal access to care and healthcare outcomes for marginalized communities.\\n2. **Lack of transparency and accountability**: Complex AI decision-making processes may be difficult to understand, making it challenging to identify and address errors or biases that could harm individuals or communities.\\n3. **Algorithmic injustice**: Certain AI systems may disproportionately affect marginalized populations, perpetuating existing health disparities.\\n4. **Informed consent**: Individuals within marginalized communities may not be fully informed about how their data is being used or by which entities, raising concerns about autonomy and trust in the decision-making process.\\n\\n**Variations across cultural contexts:**\\n\\n1. **Cultural values and norms**: Different cultures place varying emphasis on community, family ties, or individualism, and these may influence the design of AI systems and how they are implemented.\\n2. **Language barriers**: Communication between service providers and patients from diverse linguistic backgrounds can create barriers to accessing healthcare services and make it more challenging to resolve errors or disputes.\\n3. **Power dynamics**: AI decision-making processes may be influenced by power imbalances within communities, such as between healthcare providers and patients with varying levels of education or socio-economic status.\\n4. **Local customs and practices**: In some cultures, there may be unique customs and practices related to health care delivery, which AI systems should respect and adapt to ensure effective collaboration.\\n\\n**Mitigating strategies:**\\n\\n1. **Interdisciplinary collaboration**: Multidisciplinary teams will be essential in addressing the complexities of AI decision-making processes in public health.\\n2. **Cultural sensitivity training**: Providers should receive training on cultural differences and nuances related to marginalized populations.\\n3. **Inclusive data collection**: Data sets used to train AI systems should reflect diverse populations and contexts, addressing biases and disparities.\\n4. **Clear communication**: Patients and community members should be actively engaged in the decision-making process, ensuring that their concerns are heard and addressed.\\n5. **Adaptability and iterative improvement**: AI systems will require continuous evaluation, adjustment, and refinement to address emerging issues and ensure fairness and equity.\\n\\nBy acknowledging these complexities and embracing inclusive, culturally sensitive approaches, we can harness the potential of advanced AI in public health while minimizing harm to marginalized communities.', 'The use of advanced AI systems in public health decision-making holds immense promise for improving efficiency, diagnostics, and resource allocation. However, deploying these systems, especially in marginalized communities, introduces complex ethical challenges that are further complicated by varying cultural contexts.\\n\\nHere are the potential ethical implications:\\n\\n### Ethical Implications for Public Health Decision-Making in Marginalized Communities:\\n\\n1.  **Algorithmic Bias and Amplification of Health Disparities:**\\n    *   **Data Bias:** AI systems learn from historical data. If this data disproportionately represents certain groups, contains historical biases (e.g., medical racism, under-diagnosis of certain conditions in specific ethnic groups), or has gaps for marginalized communities, the AI will learn and perpetuate these biases. For instance, diagnostic AI trained primarily on data from lighter skin tones might perform poorly on darker skin.\\n    *   **Outcome Bias:** AI models can reflect and even exacerbate existing socioeconomic and health inequities. An AI recommending resource allocation might, based on historical patterns, de-prioritize communities that have been historically underserved, leading to reduced access to care or lower quality interventions.\\n    *   **Proxy Discrimination:** AI might use proxies (e.g., zip codes, education levels) that correlate with race, income, or other protected characteristics, leading to indirect discrimination, even if the system isn\\'t explicitly programmed to do so.\\n\\n2.  **Privacy and Surveillance:**\\n    *   **Excessive Data Collection:** AI systems require vast amounts of data, including sensitive health, genetic, and even behavioral information. Marginalized communities, often under greater scrutiny or with less digital literacy, might be disproportionately subject to data collection without full informed consent or understanding of its implications.\\n    *   **Re-identification Risk:** Even \"anonymized\" data can sometimes be re-identified, especially with advanced AI techniques, posing risks to individuals and groups.\\n    *   **Dual-Use Potential:** Public health data collected by AI could be misused for surveillance, immigration enforcement, or criminal justice purposes, eroding trust, particularly in communities with historical mistrust of authorities.\\n\\n3.  **Autonomy, Consent, and Paternalism:**\\n    *   **Lack of Informed Consent:** Obtaining truly informed consent for complex AI interventions can be challenging. For marginalized communities, this is compounded by language barriers, lower literacy rates, power imbalances, and historical experiences of exploitation (e.g., Tuskegee Syphilis Study).\\n    *   **Coercion/Pressure:** If AI systems become the primary decision-makers or gatekeepers for essential services, individuals might feel pressured to comply with AI recommendations even if they don\\'t fully understand or agree, eroding their autonomy.\\n    *   **Digital Divide:** Lack of access to technology, reliable internet, or digital literacy can exclude marginalized individuals from participating in or benefiting from AI-driven public health initiatives, further disempowering them.\\n\\n4.  **Accountability and Transparency (The \"Black Box\" Problem):**\\n    *   **Lack of Explainability:** Many advanced AI systems (deep learning) are \"black boxes,\" meaning their decision-making processes are opaque. When an AI system makes a critical public health decision (e.g., recommending a lockdown, triaging patients), it\\'s hard to understand *why* it made that choice.\\n    *   **Diffusion of Responsibility:** If an AI makes a wrong or harmful decision, who is accountable? The developer? The public health official who deployed it? This becomes even more complex when the impact falls disproportionately on vulnerable groups.\\n    *   **Erosion of Trust:** If public health decisions are made by inscrutable algorithms, it can foster mistrust, especially if those decisions appear to perpetuate existing injustices or discriminate against certain groups.\\n\\n5.  **Equity and Access:**\\n    *   **Exacerbating Resource Inequality:** If AI systems are costly to develop and deploy, they might be first adopted by well-resourced health systems, potentially widening the gap between areas with advanced AI tools and those without.\\n    *   **Focus on Quantifiable Metrics:** AI systems tend to optimize for quantifiable metrics. This might lead to overlooking crucial qualitative factors, social determinants of health, or community-specific needs that are harder to measure but vital for marginalized communities.\\n\\n### Variation Across Different Cultural Contexts:\\n\\nCultural context profoundly shapes how these ethical implications manifest and how acceptable AI-driven public health interventions are perceived.\\n\\n1.  **Data Privacy & Ownership:**\\n    *   **Individualistic vs. Collectivistic Cultures:** Western individualistic cultures often emphasize personal data privacy and consent. In many collectivistic cultures, data might be viewed more communally, and consent might involve family elders or community leaders rather than just the individual. However, this doesn\\'t necessarily mean less concern about privacy; it\\'s just framed differently.\\n    *   **Trust in Institutions:** Cultures vary widely in their trust in government, medical institutions, and technology companies. Low trust (e.g., due to historical colonial abuses or authoritarian regimes) will make AI adoption in public health far more challenging, regardless of the AI\\'s technical merit.\\n    *   **Indigenous Data Sovereignty:** Many Indigenous communities assert inherent rights to control their data, including health data, reflecting unique cultural understandings of information, knowledge, and self-determination.\\n\\n2.  **Concept of Health & Well-being:**\\n    *   **Biomedical vs. Holistic Views:** Many cultures have a holistic view of health, integrating physical, mental, spiritual, and community well-being. AI optimized for purely biomedical outcomes might miss or even contradict these broader cultural understandings, leading to ineffective or culturally inappropriate interventions.\\n    *   **Traditional Medicine:** In cultures where traditional or indigenous medicine plays a significant role, AI-driven public health approaches might clash with or be seen as dismissive of established healing practices.\\n\\n3.  **Decision-Making and Autonomy:**\\n    *   **Family/Community Involvement:** In many cultures, health decisions are not purely individual but involve family members, elders, or community leaders. AI systems designed for individual-centric decision-making might not integrate well into these processes.\\n    *   **Fatalism/Determinism:** Some cultural beliefs might influence attitudes towards health interventions, potentially affecting willingness to engage with AI-driven preventative or diagnostic tools.\\n\\n4.  **Communication and Language:**\\n    *   **Linguistic Diversity:** Public health interventions must be linguistically appropriate. AI systems need to be trained on diverse linguistic datasets, and their outputs must be comprehensible and culturally resonant.\\n    *   **Non-Verbal Cues:** AI relies heavily on structured data. In many cultures, non-verbal cues, storytelling, and nuanced communication are crucial for understanding health concerns and building rapport, which AI currently struggles to interpret.\\n\\n5.  **Spirituality and Belief Systems:**\\n    *   **Religious/Spiritual Influence:** Religious or spiritual beliefs can profoundly influence health behaviors, treatment choices, and attitudes towards technology. AI interventions that do not respectfully acknowledge or integrate these beliefs may face strong resistance.\\n\\nIn conclusion, while advanced AI systems offer significant potential for public health, their deployment, especially in marginalized communities and across diverse cultural contexts, requires profound ethical consideration. A human-centered, culturally sensitive, and community-engaged approach is crucial to ensure these systems genuinely improve health equity rather than exacerbating existing disparities or eroding fundamental rights and values. This necessitates diverse development teams, robust regulatory frameworks, continuous auditing for bias, and meaningful participation from the communities served.', 'The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises several potential ethical implications. Here are some possible considerations:\\n\\n1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory practices if they are trained on biased data or designed with a particular worldview. For example, an AI system that categorizes individuals based on their socioeconomic status or ethnicity may unfairly penalize or benefit marginalized communities.\\n2. **Lack of transparency and accountability**: Complex AI decision-making processes can be opaque, making it difficult to understand how decisions were made. This lack of transparency can erode trust in the system and create opportunities for abuse.\\n3. **Extrication bias**: AI systems may be trained on datasets that reflect existing power dynamics, which can lead to \"inferred\" or unintentionally biased decision-making processes.\\n4. **Limited contextual understanding**: AI systems may not fully understand the nuances of human behavior, culture, and contexts, leading to decisions that are not tailored to specific populations.\\n\\nEthical implications across different cultural contexts:\\n\\n1. **Diversity and inclusion**:\\nIn some cultures, AI systems may be seen as more trustworthy or capable than those developed in other countries due to factors like language proficiency or cultural norms.\\n2. **Contextual understanding**: The extent of contextual awareness varies across cultures, and AI systems designed for one cultural context may not perform well in others.\\n3. **Power dynamics**: In some cultures, there may be existing power imbalances that influence how AI systems are developed, deployed, or controlled.\\n4. **Linguistic and social nuances**: Cultural differences in communication styles, idioms, and social norms can affect the way people understand and interpret AI-generated outputs.\\n\\nExamples of cultural variations:\\n\\n1. In some South Asian cultures, there may be a tendency to use AI-powered systems to filter out perceived \"undesirables,\" such as individuals with disabilities or marginalized groups.\\n2. In some Indigenous communities, AI systems may be used to track land use patterns and monitor environmental changes, but these efforts may be framed in ways that prioritize European colonialism or resource extraction.\\n3. In Arab cultures, social media is highly influential in shaping public opinion, which can be leveraged by policymakers to influence AI decision-making processes.\\n\\nVarying implications across different cultural contexts:\\n\\n1. **More severe biases**: AI system design and training may exacerbate existing biases in more culturally diverse societies, perpetuating systemic inequalities.\\n2. **More complex solutions**: In contrast, cultural nuances might lead to more nuanced and context-specific AI solutions that account for power dynamics, language use, and social norms.\\n\\nTo mitigate these concerns, it is essential to:\\n\\n1. **Conduct inclusive research and testing** across various cultural contexts to ensure AI systems are developed with a deep understanding of diverse experiences.\\n2. **Engage in iterative, community-driven processes** where stakeholders collaborate on designing, deploying, and monitoring AI systems that respect cultural differences and values.\\n3. **Implement robust audit trails**, data protection policies, and transparent reporting mechanisms to maintain trust and accountability.\\n\\nUltimately, effective AI governance requires a multi-faceted approach that balances technological innovation with social responsibility, inclusivity, and responsiveness to diverse contexts and cultures.']\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gpt-4o-mini\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, especially in marginalized communities, entails several ethical implications that can vary significantly across different cultural contexts. Here are some key considerations:\n",
      "\n",
      "### 1. **Bias and Discrimination**\n",
      "- **Implication**: AI systems are often trained on historical data that may reflect existing biases, potentially leading to discriminatory outcomes against marginalized communities.\n",
      "- **Cultural Variation**: In some cultures, historical injustices may compound the effect of AI biases, making the consequences more pronounced. For instance, in a society with a history of systemic racism, biased AI algorithms could exacerbate health disparities.\n",
      "\n",
      "### 2. **Informed Consent and Autonomy**\n",
      "- **Implication**: The deployment of AI in public health decision-making can overlook the importance of informed consent, especially if marginalized communities do not fully understand the technology being used.\n",
      "- **Cultural Variation**: In cultures with different values regarding autonomy and authority, the concept of informed consent might be interpreted differently. Populations with communal decision-making traditions may view collective consent as more important than individual consent.\n",
      "\n",
      "### 3. **Transparency and Explainability**\n",
      "- **Implication**: AI systems can be \"black boxes,\" making it challenging for communities to understand how decisions affecting their health are made.\n",
      "- **Cultural Variation**: In cultures that emphasize transparency and accountability in governance, a lack of explainability could lead to mistrust towards health authorities and the technology itself, whereas in other cultures, reliance on expert opinion may reduce the demand for transparency.\n",
      "\n",
      "### 4. **Access and Equity**\n",
      "- **Implication**: Advanced AI tools might not be equally accessible to all communities, potentially widening the gap in health outcomes.\n",
      "- **Cultural Variation**: In regions with significant income inequality or disparities in technological infrastructure, marginalized communities may benefit less from AI advancements, while wealthier areas may utilize these technologies more effectively.\n",
      "\n",
      "### 5. **Data Privacy and Surveillance**\n",
      "- **Implication**: The use of AI in public health can lead to increased surveillance, raising concerns about individual privacy, especially in communities less equipped to advocate for their rights.\n",
      "- **Cultural Variation**: Societies with strong privacy laws may push back against data misuse, while in collectivist cultures, data may be viewed as a community resource, leading to conflicts over privacy versus communal benefit.\n",
      "\n",
      "### 6. **Trust in Institutions**\n",
      "- **Implication**: Trust in health institutions can be jeopardized if communities perceive AI as undermining their voices in public health decisions.\n",
      "- **Cultural Variation**: Cultures with a higher level of trust in medical or governmental authority may accept AI-driven recommendations more readily than those with historical skepticism towards these institutions.\n",
      "\n",
      "### 7. **Resilience and Capacity Building**\n",
      "- **Implication**: Relying solely on AI systems can undermine local capacities for resilience and self-determination in health interventions.\n",
      "- **Cultural Variation**: Different cultural attitudes towards technology and tradition may affect how communities rate their ability to engage with, adopt, or resist AI-driven initiatives.\n",
      "\n",
      "### 8. **Ethical Considerations in AI Development**\n",
      "- **Implication**: The ethical frameworks guiding AI design can vary significantly, potentially leading to prototypes that do not align with the values or needs of marginalized communities.\n",
      "- **Cultural Variation**: In some cultures, indigenous knowledge systems and traditional practices regarding health may conflict with the data-driven approaches of AI, raising questions about whose values and knowledge are prioritized.\n",
      "\n",
      "### Conclusion\n",
      "Addressing these ethical implications requires a culturally aware approach that involves careful consideration of local contexts, active engagement with communities, and the formulation of guidelines that emphasize equity, justice, and respect for diverse values. Policymakers and AI developers should prioritize inclusivity and participatory methods to ensure that AI tools in public health contribute positively to marginalized communities, rather than perpetuating or exacerbating existing inequalities.\n",
      "Competitor: claude-3-7-sonnet-latest\n",
      "\n",
      "# Ethical Implications of AI in Public Health Decision-Making\n",
      "\n",
      "## Core Ethical Considerations\n",
      "\n",
      "**Algorithmic Bias and Health Disparities**\n",
      "AI systems trained on historically biased data may perpetuate or amplify existing health inequities in marginalized communities. This could manifest in resource misallocation, misdiagnosis, or inappropriate treatment recommendations.\n",
      "\n",
      "**Autonomy and Informed Consent**\n",
      "Questions arise about who controls health decisions when AI systems are involved, particularly in communities with limited technological literacy or where cultural concepts of autonomy differ from Western frameworks.\n",
      "\n",
      "**Transparency and Explainability**\n",
      "\"Black box\" AI systems may make critical health decisions without clear explanations, limiting accountability and trust, especially in communities with historical reasons to distrust healthcare authorities.\n",
      "\n",
      "## Cultural Context Variations\n",
      "\n",
      "**Collectivist vs. Individualist Societies**\n",
      "- In collectivist cultures, AI systems designed with Western individualistic assumptions about privacy and decision-making may conflict with community-centered approaches to health.\n",
      "- Different cultures may have varying comfort levels with technological intervention in health decisions.\n",
      "\n",
      "**Religious and Traditional Frameworks**\n",
      "Health decisions in many communities are guided by religious or traditional practices that AI systems may not adequately account for or may actively contradict.\n",
      "\n",
      "**Power Dynamics**\n",
      "In postcolonial contexts or regions with histories of medical exploitation, AI systems deployed by external entities may be perceived as continuing patterns of extraction and control rather than empowerment.\n",
      "\n",
      "## Mitigation Approaches\n",
      "\n",
      "Effective approaches would include community co-design of AI systems, cultural competency requirements in AI development, and governance frameworks that prioritize local autonomy while benefiting from AI capabilities.\n",
      "Competitor: llama3.2:1b\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises significant ethical concerns. Here are some potential implications and considerations that may vary across different cultural contexts:\n",
      "\n",
      "**Ethical concerns:**\n",
      "\n",
      "1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory patterns if they are trained on biased data or designed with a discriminatory goal. This can lead to unequal access to care and healthcare outcomes for marginalized communities.\n",
      "2. **Lack of transparency and accountability**: Complex AI decision-making processes may be difficult to understand, making it challenging to identify and address errors or biases that could harm individuals or communities.\n",
      "3. **Algorithmic injustice**: Certain AI systems may disproportionately affect marginalized populations, perpetuating existing health disparities.\n",
      "4. **Informed consent**: Individuals within marginalized communities may not be fully informed about how their data is being used or by which entities, raising concerns about autonomy and trust in the decision-making process.\n",
      "\n",
      "**Variations across cultural contexts:**\n",
      "\n",
      "1. **Cultural values and norms**: Different cultures place varying emphasis on community, family ties, or individualism, and these may influence the design of AI systems and how they are implemented.\n",
      "2. **Language barriers**: Communication between service providers and patients from diverse linguistic backgrounds can create barriers to accessing healthcare services and make it more challenging to resolve errors or disputes.\n",
      "3. **Power dynamics**: AI decision-making processes may be influenced by power imbalances within communities, such as between healthcare providers and patients with varying levels of education or socio-economic status.\n",
      "4. **Local customs and practices**: In some cultures, there may be unique customs and practices related to health care delivery, which AI systems should respect and adapt to ensure effective collaboration.\n",
      "\n",
      "**Mitigating strategies:**\n",
      "\n",
      "1. **Interdisciplinary collaboration**: Multidisciplinary teams will be essential in addressing the complexities of AI decision-making processes in public health.\n",
      "2. **Cultural sensitivity training**: Providers should receive training on cultural differences and nuances related to marginalized populations.\n",
      "3. **Inclusive data collection**: Data sets used to train AI systems should reflect diverse populations and contexts, addressing biases and disparities.\n",
      "4. **Clear communication**: Patients and community members should be actively engaged in the decision-making process, ensuring that their concerns are heard and addressed.\n",
      "5. **Adaptability and iterative improvement**: AI systems will require continuous evaluation, adjustment, and refinement to address emerging issues and ensure fairness and equity.\n",
      "\n",
      "By acknowledging these complexities and embracing inclusive, culturally sensitive approaches, we can harness the potential of advanced AI in public health while minimizing harm to marginalized communities.\n",
      "Competitor: gemini-2.5-flash-preview-05-20\n",
      "\n",
      "The use of advanced AI systems in public health decision-making holds immense promise for improving efficiency, diagnostics, and resource allocation. However, deploying these systems, especially in marginalized communities, introduces complex ethical challenges that are further complicated by varying cultural contexts.\n",
      "\n",
      "Here are the potential ethical implications:\n",
      "\n",
      "### Ethical Implications for Public Health Decision-Making in Marginalized Communities:\n",
      "\n",
      "1.  **Algorithmic Bias and Amplification of Health Disparities:**\n",
      "    *   **Data Bias:** AI systems learn from historical data. If this data disproportionately represents certain groups, contains historical biases (e.g., medical racism, under-diagnosis of certain conditions in specific ethnic groups), or has gaps for marginalized communities, the AI will learn and perpetuate these biases. For instance, diagnostic AI trained primarily on data from lighter skin tones might perform poorly on darker skin.\n",
      "    *   **Outcome Bias:** AI models can reflect and even exacerbate existing socioeconomic and health inequities. An AI recommending resource allocation might, based on historical patterns, de-prioritize communities that have been historically underserved, leading to reduced access to care or lower quality interventions.\n",
      "    *   **Proxy Discrimination:** AI might use proxies (e.g., zip codes, education levels) that correlate with race, income, or other protected characteristics, leading to indirect discrimination, even if the system isn't explicitly programmed to do so.\n",
      "\n",
      "2.  **Privacy and Surveillance:**\n",
      "    *   **Excessive Data Collection:** AI systems require vast amounts of data, including sensitive health, genetic, and even behavioral information. Marginalized communities, often under greater scrutiny or with less digital literacy, might be disproportionately subject to data collection without full informed consent or understanding of its implications.\n",
      "    *   **Re-identification Risk:** Even \"anonymized\" data can sometimes be re-identified, especially with advanced AI techniques, posing risks to individuals and groups.\n",
      "    *   **Dual-Use Potential:** Public health data collected by AI could be misused for surveillance, immigration enforcement, or criminal justice purposes, eroding trust, particularly in communities with historical mistrust of authorities.\n",
      "\n",
      "3.  **Autonomy, Consent, and Paternalism:**\n",
      "    *   **Lack of Informed Consent:** Obtaining truly informed consent for complex AI interventions can be challenging. For marginalized communities, this is compounded by language barriers, lower literacy rates, power imbalances, and historical experiences of exploitation (e.g., Tuskegee Syphilis Study).\n",
      "    *   **Coercion/Pressure:** If AI systems become the primary decision-makers or gatekeepers for essential services, individuals might feel pressured to comply with AI recommendations even if they don't fully understand or agree, eroding their autonomy.\n",
      "    *   **Digital Divide:** Lack of access to technology, reliable internet, or digital literacy can exclude marginalized individuals from participating in or benefiting from AI-driven public health initiatives, further disempowering them.\n",
      "\n",
      "4.  **Accountability and Transparency (The \"Black Box\" Problem):**\n",
      "    *   **Lack of Explainability:** Many advanced AI systems (deep learning) are \"black boxes,\" meaning their decision-making processes are opaque. When an AI system makes a critical public health decision (e.g., recommending a lockdown, triaging patients), it's hard to understand *why* it made that choice.\n",
      "    *   **Diffusion of Responsibility:** If an AI makes a wrong or harmful decision, who is accountable? The developer? The public health official who deployed it? This becomes even more complex when the impact falls disproportionately on vulnerable groups.\n",
      "    *   **Erosion of Trust:** If public health decisions are made by inscrutable algorithms, it can foster mistrust, especially if those decisions appear to perpetuate existing injustices or discriminate against certain groups.\n",
      "\n",
      "5.  **Equity and Access:**\n",
      "    *   **Exacerbating Resource Inequality:** If AI systems are costly to develop and deploy, they might be first adopted by well-resourced health systems, potentially widening the gap between areas with advanced AI tools and those without.\n",
      "    *   **Focus on Quantifiable Metrics:** AI systems tend to optimize for quantifiable metrics. This might lead to overlooking crucial qualitative factors, social determinants of health, or community-specific needs that are harder to measure but vital for marginalized communities.\n",
      "\n",
      "### Variation Across Different Cultural Contexts:\n",
      "\n",
      "Cultural context profoundly shapes how these ethical implications manifest and how acceptable AI-driven public health interventions are perceived.\n",
      "\n",
      "1.  **Data Privacy & Ownership:**\n",
      "    *   **Individualistic vs. Collectivistic Cultures:** Western individualistic cultures often emphasize personal data privacy and consent. In many collectivistic cultures, data might be viewed more communally, and consent might involve family elders or community leaders rather than just the individual. However, this doesn't necessarily mean less concern about privacy; it's just framed differently.\n",
      "    *   **Trust in Institutions:** Cultures vary widely in their trust in government, medical institutions, and technology companies. Low trust (e.g., due to historical colonial abuses or authoritarian regimes) will make AI adoption in public health far more challenging, regardless of the AI's technical merit.\n",
      "    *   **Indigenous Data Sovereignty:** Many Indigenous communities assert inherent rights to control their data, including health data, reflecting unique cultural understandings of information, knowledge, and self-determination.\n",
      "\n",
      "2.  **Concept of Health & Well-being:**\n",
      "    *   **Biomedical vs. Holistic Views:** Many cultures have a holistic view of health, integrating physical, mental, spiritual, and community well-being. AI optimized for purely biomedical outcomes might miss or even contradict these broader cultural understandings, leading to ineffective or culturally inappropriate interventions.\n",
      "    *   **Traditional Medicine:** In cultures where traditional or indigenous medicine plays a significant role, AI-driven public health approaches might clash with or be seen as dismissive of established healing practices.\n",
      "\n",
      "3.  **Decision-Making and Autonomy:**\n",
      "    *   **Family/Community Involvement:** In many cultures, health decisions are not purely individual but involve family members, elders, or community leaders. AI systems designed for individual-centric decision-making might not integrate well into these processes.\n",
      "    *   **Fatalism/Determinism:** Some cultural beliefs might influence attitudes towards health interventions, potentially affecting willingness to engage with AI-driven preventative or diagnostic tools.\n",
      "\n",
      "4.  **Communication and Language:**\n",
      "    *   **Linguistic Diversity:** Public health interventions must be linguistically appropriate. AI systems need to be trained on diverse linguistic datasets, and their outputs must be comprehensible and culturally resonant.\n",
      "    *   **Non-Verbal Cues:** AI relies heavily on structured data. In many cultures, non-verbal cues, storytelling, and nuanced communication are crucial for understanding health concerns and building rapport, which AI currently struggles to interpret.\n",
      "\n",
      "5.  **Spirituality and Belief Systems:**\n",
      "    *   **Religious/Spiritual Influence:** Religious or spiritual beliefs can profoundly influence health behaviors, treatment choices, and attitudes towards technology. AI interventions that do not respectfully acknowledge or integrate these beliefs may face strong resistance.\n",
      "\n",
      "In conclusion, while advanced AI systems offer significant potential for public health, their deployment, especially in marginalized communities and across diverse cultural contexts, requires profound ethical consideration. A human-centered, culturally sensitive, and community-engaged approach is crucial to ensure these systems genuinely improve health equity rather than exacerbating existing disparities or eroding fundamental rights and values. This necessitates diverse development teams, robust regulatory frameworks, continuous auditing for bias, and meaningful participation from the communities served.\n",
      "Competitor: llama3.2:1b\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises several potential ethical implications. Here are some possible considerations:\n",
      "\n",
      "1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory practices if they are trained on biased data or designed with a particular worldview. For example, an AI system that categorizes individuals based on their socioeconomic status or ethnicity may unfairly penalize or benefit marginalized communities.\n",
      "2. **Lack of transparency and accountability**: Complex AI decision-making processes can be opaque, making it difficult to understand how decisions were made. This lack of transparency can erode trust in the system and create opportunities for abuse.\n",
      "3. **Extrication bias**: AI systems may be trained on datasets that reflect existing power dynamics, which can lead to \"inferred\" or unintentionally biased decision-making processes.\n",
      "4. **Limited contextual understanding**: AI systems may not fully understand the nuances of human behavior, culture, and contexts, leading to decisions that are not tailored to specific populations.\n",
      "\n",
      "Ethical implications across different cultural contexts:\n",
      "\n",
      "1. **Diversity and inclusion**:\n",
      "In some cultures, AI systems may be seen as more trustworthy or capable than those developed in other countries due to factors like language proficiency or cultural norms.\n",
      "2. **Contextual understanding**: The extent of contextual awareness varies across cultures, and AI systems designed for one cultural context may not perform well in others.\n",
      "3. **Power dynamics**: In some cultures, there may be existing power imbalances that influence how AI systems are developed, deployed, or controlled.\n",
      "4. **Linguistic and social nuances**: Cultural differences in communication styles, idioms, and social norms can affect the way people understand and interpret AI-generated outputs.\n",
      "\n",
      "Examples of cultural variations:\n",
      "\n",
      "1. In some South Asian cultures, there may be a tendency to use AI-powered systems to filter out perceived \"undesirables,\" such as individuals with disabilities or marginalized groups.\n",
      "2. In some Indigenous communities, AI systems may be used to track land use patterns and monitor environmental changes, but these efforts may be framed in ways that prioritize European colonialism or resource extraction.\n",
      "3. In Arab cultures, social media is highly influential in shaping public opinion, which can be leveraged by policymakers to influence AI decision-making processes.\n",
      "\n",
      "Varying implications across different cultural contexts:\n",
      "\n",
      "1. **More severe biases**: AI system design and training may exacerbate existing biases in more culturally diverse societies, perpetuating systemic inequalities.\n",
      "2. **More complex solutions**: In contrast, cultural nuances might lead to more nuanced and context-specific AI solutions that account for power dynamics, language use, and social norms.\n",
      "\n",
      "To mitigate these concerns, it is essential to:\n",
      "\n",
      "1. **Conduct inclusive research and testing** across various cultural contexts to ensure AI systems are developed with a deep understanding of diverse experiences.\n",
      "2. **Engage in iterative, community-driven processes** where stakeholders collaborate on designing, deploying, and monitoring AI systems that respect cultural differences and values.\n",
      "3. **Implement robust audit trails**, data protection policies, and transparent reporting mechanisms to maintain trust and accountability.\n",
      "\n",
      "Ultimately, effective AI governance requires a multi-faceted approach that balances technological innovation with social responsibility, inclusivity, and responsiveness to diverse contexts and cultures.\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, especially in marginalized communities, entails several ethical implications that can vary significantly across different cultural contexts. Here are some key considerations:\n",
      "\n",
      "### 1. **Bias and Discrimination**\n",
      "- **Implication**: AI systems are often trained on historical data that may reflect existing biases, potentially leading to discriminatory outcomes against marginalized communities.\n",
      "- **Cultural Variation**: In some cultures, historical injustices may compound the effect of AI biases, making the consequences more pronounced. For instance, in a society with a history of systemic racism, biased AI algorithms could exacerbate health disparities.\n",
      "\n",
      "### 2. **Informed Consent and Autonomy**\n",
      "- **Implication**: The deployment of AI in public health decision-making can overlook the importance of informed consent, especially if marginalized communities do not fully understand the technology being used.\n",
      "- **Cultural Variation**: In cultures with different values regarding autonomy and authority, the concept of informed consent might be interpreted differently. Populations with communal decision-making traditions may view collective consent as more important than individual consent.\n",
      "\n",
      "### 3. **Transparency and Explainability**\n",
      "- **Implication**: AI systems can be \"black boxes,\" making it challenging for communities to understand how decisions affecting their health are made.\n",
      "- **Cultural Variation**: In cultures that emphasize transparency and accountability in governance, a lack of explainability could lead to mistrust towards health authorities and the technology itself, whereas in other cultures, reliance on expert opinion may reduce the demand for transparency.\n",
      "\n",
      "### 4. **Access and Equity**\n",
      "- **Implication**: Advanced AI tools might not be equally accessible to all communities, potentially widening the gap in health outcomes.\n",
      "- **Cultural Variation**: In regions with significant income inequality or disparities in technological infrastructure, marginalized communities may benefit less from AI advancements, while wealthier areas may utilize these technologies more effectively.\n",
      "\n",
      "### 5. **Data Privacy and Surveillance**\n",
      "- **Implication**: The use of AI in public health can lead to increased surveillance, raising concerns about individual privacy, especially in communities less equipped to advocate for their rights.\n",
      "- **Cultural Variation**: Societies with strong privacy laws may push back against data misuse, while in collectivist cultures, data may be viewed as a community resource, leading to conflicts over privacy versus communal benefit.\n",
      "\n",
      "### 6. **Trust in Institutions**\n",
      "- **Implication**: Trust in health institutions can be jeopardized if communities perceive AI as undermining their voices in public health decisions.\n",
      "- **Cultural Variation**: Cultures with a higher level of trust in medical or governmental authority may accept AI-driven recommendations more readily than those with historical skepticism towards these institutions.\n",
      "\n",
      "### 7. **Resilience and Capacity Building**\n",
      "- **Implication**: Relying solely on AI systems can undermine local capacities for resilience and self-determination in health interventions.\n",
      "- **Cultural Variation**: Different cultural attitudes towards technology and tradition may affect how communities rate their ability to engage with, adopt, or resist AI-driven initiatives.\n",
      "\n",
      "### 8. **Ethical Considerations in AI Development**\n",
      "- **Implication**: The ethical frameworks guiding AI design can vary significantly, potentially leading to prototypes that do not align with the values or needs of marginalized communities.\n",
      "- **Cultural Variation**: In some cultures, indigenous knowledge systems and traditional practices regarding health may conflict with the data-driven approaches of AI, raising questions about whose values and knowledge are prioritized.\n",
      "\n",
      "### Conclusion\n",
      "Addressing these ethical implications requires a culturally aware approach that involves careful consideration of local contexts, active engagement with communities, and the formulation of guidelines that emphasize equity, justice, and respect for diverse values. Policymakers and AI developers should prioritize inclusivity and participatory methods to ensure that AI tools in public health contribute positively to marginalized communities, rather than perpetuating or exacerbating existing inequalities.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "# Ethical Implications of AI in Public Health Decision-Making\n",
      "\n",
      "## Core Ethical Considerations\n",
      "\n",
      "**Algorithmic Bias and Health Disparities**\n",
      "AI systems trained on historically biased data may perpetuate or amplify existing health inequities in marginalized communities. This could manifest in resource misallocation, misdiagnosis, or inappropriate treatment recommendations.\n",
      "\n",
      "**Autonomy and Informed Consent**\n",
      "Questions arise about who controls health decisions when AI systems are involved, particularly in communities with limited technological literacy or where cultural concepts of autonomy differ from Western frameworks.\n",
      "\n",
      "**Transparency and Explainability**\n",
      "\"Black box\" AI systems may make critical health decisions without clear explanations, limiting accountability and trust, especially in communities with historical reasons to distrust healthcare authorities.\n",
      "\n",
      "## Cultural Context Variations\n",
      "\n",
      "**Collectivist vs. Individualist Societies**\n",
      "- In collectivist cultures, AI systems designed with Western individualistic assumptions about privacy and decision-making may conflict with community-centered approaches to health.\n",
      "- Different cultures may have varying comfort levels with technological intervention in health decisions.\n",
      "\n",
      "**Religious and Traditional Frameworks**\n",
      "Health decisions in many communities are guided by religious or traditional practices that AI systems may not adequately account for or may actively contradict.\n",
      "\n",
      "**Power Dynamics**\n",
      "In postcolonial contexts or regions with histories of medical exploitation, AI systems deployed by external entities may be perceived as continuing patterns of extraction and control rather than empowerment.\n",
      "\n",
      "## Mitigation Approaches\n",
      "\n",
      "Effective approaches would include community co-design of AI systems, cultural competency requirements in AI development, and governance frameworks that prioritize local autonomy while benefiting from AI capabilities.\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises significant ethical concerns. Here are some potential implications and considerations that may vary across different cultural contexts:\n",
      "\n",
      "**Ethical concerns:**\n",
      "\n",
      "1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory patterns if they are trained on biased data or designed with a discriminatory goal. This can lead to unequal access to care and healthcare outcomes for marginalized communities.\n",
      "2. **Lack of transparency and accountability**: Complex AI decision-making processes may be difficult to understand, making it challenging to identify and address errors or biases that could harm individuals or communities.\n",
      "3. **Algorithmic injustice**: Certain AI systems may disproportionately affect marginalized populations, perpetuating existing health disparities.\n",
      "4. **Informed consent**: Individuals within marginalized communities may not be fully informed about how their data is being used or by which entities, raising concerns about autonomy and trust in the decision-making process.\n",
      "\n",
      "**Variations across cultural contexts:**\n",
      "\n",
      "1. **Cultural values and norms**: Different cultures place varying emphasis on community, family ties, or individualism, and these may influence the design of AI systems and how they are implemented.\n",
      "2. **Language barriers**: Communication between service providers and patients from diverse linguistic backgrounds can create barriers to accessing healthcare services and make it more challenging to resolve errors or disputes.\n",
      "3. **Power dynamics**: AI decision-making processes may be influenced by power imbalances within communities, such as between healthcare providers and patients with varying levels of education or socio-economic status.\n",
      "4. **Local customs and practices**: In some cultures, there may be unique customs and practices related to health care delivery, which AI systems should respect and adapt to ensure effective collaboration.\n",
      "\n",
      "**Mitigating strategies:**\n",
      "\n",
      "1. **Interdisciplinary collaboration**: Multidisciplinary teams will be essential in addressing the complexities of AI decision-making processes in public health.\n",
      "2. **Cultural sensitivity training**: Providers should receive training on cultural differences and nuances related to marginalized populations.\n",
      "3. **Inclusive data collection**: Data sets used to train AI systems should reflect diverse populations and contexts, addressing biases and disparities.\n",
      "4. **Clear communication**: Patients and community members should be actively engaged in the decision-making process, ensuring that their concerns are heard and addressed.\n",
      "5. **Adaptability and iterative improvement**: AI systems will require continuous evaluation, adjustment, and refinement to address emerging issues and ensure fairness and equity.\n",
      "\n",
      "By acknowledging these complexities and embracing inclusive, culturally sensitive approaches, we can harness the potential of advanced AI in public health while minimizing harm to marginalized communities.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "The use of advanced AI systems in public health decision-making holds immense promise for improving efficiency, diagnostics, and resource allocation. However, deploying these systems, especially in marginalized communities, introduces complex ethical challenges that are further complicated by varying cultural contexts.\n",
      "\n",
      "Here are the potential ethical implications:\n",
      "\n",
      "### Ethical Implications for Public Health Decision-Making in Marginalized Communities:\n",
      "\n",
      "1.  **Algorithmic Bias and Amplification of Health Disparities:**\n",
      "    *   **Data Bias:** AI systems learn from historical data. If this data disproportionately represents certain groups, contains historical biases (e.g., medical racism, under-diagnosis of certain conditions in specific ethnic groups), or has gaps for marginalized communities, the AI will learn and perpetuate these biases. For instance, diagnostic AI trained primarily on data from lighter skin tones might perform poorly on darker skin.\n",
      "    *   **Outcome Bias:** AI models can reflect and even exacerbate existing socioeconomic and health inequities. An AI recommending resource allocation might, based on historical patterns, de-prioritize communities that have been historically underserved, leading to reduced access to care or lower quality interventions.\n",
      "    *   **Proxy Discrimination:** AI might use proxies (e.g., zip codes, education levels) that correlate with race, income, or other protected characteristics, leading to indirect discrimination, even if the system isn't explicitly programmed to do so.\n",
      "\n",
      "2.  **Privacy and Surveillance:**\n",
      "    *   **Excessive Data Collection:** AI systems require vast amounts of data, including sensitive health, genetic, and even behavioral information. Marginalized communities, often under greater scrutiny or with less digital literacy, might be disproportionately subject to data collection without full informed consent or understanding of its implications.\n",
      "    *   **Re-identification Risk:** Even \"anonymized\" data can sometimes be re-identified, especially with advanced AI techniques, posing risks to individuals and groups.\n",
      "    *   **Dual-Use Potential:** Public health data collected by AI could be misused for surveillance, immigration enforcement, or criminal justice purposes, eroding trust, particularly in communities with historical mistrust of authorities.\n",
      "\n",
      "3.  **Autonomy, Consent, and Paternalism:**\n",
      "    *   **Lack of Informed Consent:** Obtaining truly informed consent for complex AI interventions can be challenging. For marginalized communities, this is compounded by language barriers, lower literacy rates, power imbalances, and historical experiences of exploitation (e.g., Tuskegee Syphilis Study).\n",
      "    *   **Coercion/Pressure:** If AI systems become the primary decision-makers or gatekeepers for essential services, individuals might feel pressured to comply with AI recommendations even if they don't fully understand or agree, eroding their autonomy.\n",
      "    *   **Digital Divide:** Lack of access to technology, reliable internet, or digital literacy can exclude marginalized individuals from participating in or benefiting from AI-driven public health initiatives, further disempowering them.\n",
      "\n",
      "4.  **Accountability and Transparency (The \"Black Box\" Problem):**\n",
      "    *   **Lack of Explainability:** Many advanced AI systems (deep learning) are \"black boxes,\" meaning their decision-making processes are opaque. When an AI system makes a critical public health decision (e.g., recommending a lockdown, triaging patients), it's hard to understand *why* it made that choice.\n",
      "    *   **Diffusion of Responsibility:** If an AI makes a wrong or harmful decision, who is accountable? The developer? The public health official who deployed it? This becomes even more complex when the impact falls disproportionately on vulnerable groups.\n",
      "    *   **Erosion of Trust:** If public health decisions are made by inscrutable algorithms, it can foster mistrust, especially if those decisions appear to perpetuate existing injustices or discriminate against certain groups.\n",
      "\n",
      "5.  **Equity and Access:**\n",
      "    *   **Exacerbating Resource Inequality:** If AI systems are costly to develop and deploy, they might be first adopted by well-resourced health systems, potentially widening the gap between areas with advanced AI tools and those without.\n",
      "    *   **Focus on Quantifiable Metrics:** AI systems tend to optimize for quantifiable metrics. This might lead to overlooking crucial qualitative factors, social determinants of health, or community-specific needs that are harder to measure but vital for marginalized communities.\n",
      "\n",
      "### Variation Across Different Cultural Contexts:\n",
      "\n",
      "Cultural context profoundly shapes how these ethical implications manifest and how acceptable AI-driven public health interventions are perceived.\n",
      "\n",
      "1.  **Data Privacy & Ownership:**\n",
      "    *   **Individualistic vs. Collectivistic Cultures:** Western individualistic cultures often emphasize personal data privacy and consent. In many collectivistic cultures, data might be viewed more communally, and consent might involve family elders or community leaders rather than just the individual. However, this doesn't necessarily mean less concern about privacy; it's just framed differently.\n",
      "    *   **Trust in Institutions:** Cultures vary widely in their trust in government, medical institutions, and technology companies. Low trust (e.g., due to historical colonial abuses or authoritarian regimes) will make AI adoption in public health far more challenging, regardless of the AI's technical merit.\n",
      "    *   **Indigenous Data Sovereignty:** Many Indigenous communities assert inherent rights to control their data, including health data, reflecting unique cultural understandings of information, knowledge, and self-determination.\n",
      "\n",
      "2.  **Concept of Health & Well-being:**\n",
      "    *   **Biomedical vs. Holistic Views:** Many cultures have a holistic view of health, integrating physical, mental, spiritual, and community well-being. AI optimized for purely biomedical outcomes might miss or even contradict these broader cultural understandings, leading to ineffective or culturally inappropriate interventions.\n",
      "    *   **Traditional Medicine:** In cultures where traditional or indigenous medicine plays a significant role, AI-driven public health approaches might clash with or be seen as dismissive of established healing practices.\n",
      "\n",
      "3.  **Decision-Making and Autonomy:**\n",
      "    *   **Family/Community Involvement:** In many cultures, health decisions are not purely individual but involve family members, elders, or community leaders. AI systems designed for individual-centric decision-making might not integrate well into these processes.\n",
      "    *   **Fatalism/Determinism:** Some cultural beliefs might influence attitudes towards health interventions, potentially affecting willingness to engage with AI-driven preventative or diagnostic tools.\n",
      "\n",
      "4.  **Communication and Language:**\n",
      "    *   **Linguistic Diversity:** Public health interventions must be linguistically appropriate. AI systems need to be trained on diverse linguistic datasets, and their outputs must be comprehensible and culturally resonant.\n",
      "    *   **Non-Verbal Cues:** AI relies heavily on structured data. In many cultures, non-verbal cues, storytelling, and nuanced communication are crucial for understanding health concerns and building rapport, which AI currently struggles to interpret.\n",
      "\n",
      "5.  **Spirituality and Belief Systems:**\n",
      "    *   **Religious/Spiritual Influence:** Religious or spiritual beliefs can profoundly influence health behaviors, treatment choices, and attitudes towards technology. AI interventions that do not respectfully acknowledge or integrate these beliefs may face strong resistance.\n",
      "\n",
      "In conclusion, while advanced AI systems offer significant potential for public health, their deployment, especially in marginalized communities and across diverse cultural contexts, requires profound ethical consideration. A human-centered, culturally sensitive, and community-engaged approach is crucial to ensure these systems genuinely improve health equity rather than exacerbating existing disparities or eroding fundamental rights and values. This necessitates diverse development teams, robust regulatory frameworks, continuous auditing for bias, and meaningful participation from the communities served.\n",
      "\n",
      "# Response from competitor 5\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises several potential ethical implications. Here are some possible considerations:\n",
      "\n",
      "1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory practices if they are trained on biased data or designed with a particular worldview. For example, an AI system that categorizes individuals based on their socioeconomic status or ethnicity may unfairly penalize or benefit marginalized communities.\n",
      "2. **Lack of transparency and accountability**: Complex AI decision-making processes can be opaque, making it difficult to understand how decisions were made. This lack of transparency can erode trust in the system and create opportunities for abuse.\n",
      "3. **Extrication bias**: AI systems may be trained on datasets that reflect existing power dynamics, which can lead to \"inferred\" or unintentionally biased decision-making processes.\n",
      "4. **Limited contextual understanding**: AI systems may not fully understand the nuances of human behavior, culture, and contexts, leading to decisions that are not tailored to specific populations.\n",
      "\n",
      "Ethical implications across different cultural contexts:\n",
      "\n",
      "1. **Diversity and inclusion**:\n",
      "In some cultures, AI systems may be seen as more trustworthy or capable than those developed in other countries due to factors like language proficiency or cultural norms.\n",
      "2. **Contextual understanding**: The extent of contextual awareness varies across cultures, and AI systems designed for one cultural context may not perform well in others.\n",
      "3. **Power dynamics**: In some cultures, there may be existing power imbalances that influence how AI systems are developed, deployed, or controlled.\n",
      "4. **Linguistic and social nuances**: Cultural differences in communication styles, idioms, and social norms can affect the way people understand and interpret AI-generated outputs.\n",
      "\n",
      "Examples of cultural variations:\n",
      "\n",
      "1. In some South Asian cultures, there may be a tendency to use AI-powered systems to filter out perceived \"undesirables,\" such as individuals with disabilities or marginalized groups.\n",
      "2. In some Indigenous communities, AI systems may be used to track land use patterns and monitor environmental changes, but these efforts may be framed in ways that prioritize European colonialism or resource extraction.\n",
      "3. In Arab cultures, social media is highly influential in shaping public opinion, which can be leveraged by policymakers to influence AI decision-making processes.\n",
      "\n",
      "Varying implications across different cultural contexts:\n",
      "\n",
      "1. **More severe biases**: AI system design and training may exacerbate existing biases in more culturally diverse societies, perpetuating systemic inequalities.\n",
      "2. **More complex solutions**: In contrast, cultural nuances might lead to more nuanced and context-specific AI solutions that account for power dynamics, language use, and social norms.\n",
      "\n",
      "To mitigate these concerns, it is essential to:\n",
      "\n",
      "1. **Conduct inclusive research and testing** across various cultural contexts to ensure AI systems are developed with a deep understanding of diverse experiences.\n",
      "2. **Engage in iterative, community-driven processes** where stakeholders collaborate on designing, deploying, and monitoring AI systems that respect cultural differences and values.\n",
      "3. **Implement robust audit trails**, data protection policies, and transparent reporting mechanisms to maintain trust and accountability.\n",
      "\n",
      "Ultimately, effective AI governance requires a multi-faceted approach that balances technological innovation with social responsibility, inclusivity, and responsiveness to diverse contexts and cultures.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 5 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "What are the potential ethical implications of using advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, and how might these implications vary across different cultural contexts?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, especially in marginalized communities, entails several ethical implications that can vary significantly across different cultural contexts. Here are some key considerations:\n",
      "\n",
      "### 1. **Bias and Discrimination**\n",
      "- **Implication**: AI systems are often trained on historical data that may reflect existing biases, potentially leading to discriminatory outcomes against marginalized communities.\n",
      "- **Cultural Variation**: In some cultures, historical injustices may compound the effect of AI biases, making the consequences more pronounced. For instance, in a society with a history of systemic racism, biased AI algorithms could exacerbate health disparities.\n",
      "\n",
      "### 2. **Informed Consent and Autonomy**\n",
      "- **Implication**: The deployment of AI in public health decision-making can overlook the importance of informed consent, especially if marginalized communities do not fully understand the technology being used.\n",
      "- **Cultural Variation**: In cultures with different values regarding autonomy and authority, the concept of informed consent might be interpreted differently. Populations with communal decision-making traditions may view collective consent as more important than individual consent.\n",
      "\n",
      "### 3. **Transparency and Explainability**\n",
      "- **Implication**: AI systems can be \"black boxes,\" making it challenging for communities to understand how decisions affecting their health are made.\n",
      "- **Cultural Variation**: In cultures that emphasize transparency and accountability in governance, a lack of explainability could lead to mistrust towards health authorities and the technology itself, whereas in other cultures, reliance on expert opinion may reduce the demand for transparency.\n",
      "\n",
      "### 4. **Access and Equity**\n",
      "- **Implication**: Advanced AI tools might not be equally accessible to all communities, potentially widening the gap in health outcomes.\n",
      "- **Cultural Variation**: In regions with significant income inequality or disparities in technological infrastructure, marginalized communities may benefit less from AI advancements, while wealthier areas may utilize these technologies more effectively.\n",
      "\n",
      "### 5. **Data Privacy and Surveillance**\n",
      "- **Implication**: The use of AI in public health can lead to increased surveillance, raising concerns about individual privacy, especially in communities less equipped to advocate for their rights.\n",
      "- **Cultural Variation**: Societies with strong privacy laws may push back against data misuse, while in collectivist cultures, data may be viewed as a community resource, leading to conflicts over privacy versus communal benefit.\n",
      "\n",
      "### 6. **Trust in Institutions**\n",
      "- **Implication**: Trust in health institutions can be jeopardized if communities perceive AI as undermining their voices in public health decisions.\n",
      "- **Cultural Variation**: Cultures with a higher level of trust in medical or governmental authority may accept AI-driven recommendations more readily than those with historical skepticism towards these institutions.\n",
      "\n",
      "### 7. **Resilience and Capacity Building**\n",
      "- **Implication**: Relying solely on AI systems can undermine local capacities for resilience and self-determination in health interventions.\n",
      "- **Cultural Variation**: Different cultural attitudes towards technology and tradition may affect how communities rate their ability to engage with, adopt, or resist AI-driven initiatives.\n",
      "\n",
      "### 8. **Ethical Considerations in AI Development**\n",
      "- **Implication**: The ethical frameworks guiding AI design can vary significantly, potentially leading to prototypes that do not align with the values or needs of marginalized communities.\n",
      "- **Cultural Variation**: In some cultures, indigenous knowledge systems and traditional practices regarding health may conflict with the data-driven approaches of AI, raising questions about whose values and knowledge are prioritized.\n",
      "\n",
      "### Conclusion\n",
      "Addressing these ethical implications requires a culturally aware approach that involves careful consideration of local contexts, active engagement with communities, and the formulation of guidelines that emphasize equity, justice, and respect for diverse values. Policymakers and AI developers should prioritize inclusivity and participatory methods to ensure that AI tools in public health contribute positively to marginalized communities, rather than perpetuating or exacerbating existing inequalities.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "# Ethical Implications of AI in Public Health Decision-Making\n",
      "\n",
      "## Core Ethical Considerations\n",
      "\n",
      "**Algorithmic Bias and Health Disparities**\n",
      "AI systems trained on historically biased data may perpetuate or amplify existing health inequities in marginalized communities. This could manifest in resource misallocation, misdiagnosis, or inappropriate treatment recommendations.\n",
      "\n",
      "**Autonomy and Informed Consent**\n",
      "Questions arise about who controls health decisions when AI systems are involved, particularly in communities with limited technological literacy or where cultural concepts of autonomy differ from Western frameworks.\n",
      "\n",
      "**Transparency and Explainability**\n",
      "\"Black box\" AI systems may make critical health decisions without clear explanations, limiting accountability and trust, especially in communities with historical reasons to distrust healthcare authorities.\n",
      "\n",
      "## Cultural Context Variations\n",
      "\n",
      "**Collectivist vs. Individualist Societies**\n",
      "- In collectivist cultures, AI systems designed with Western individualistic assumptions about privacy and decision-making may conflict with community-centered approaches to health.\n",
      "- Different cultures may have varying comfort levels with technological intervention in health decisions.\n",
      "\n",
      "**Religious and Traditional Frameworks**\n",
      "Health decisions in many communities are guided by religious or traditional practices that AI systems may not adequately account for or may actively contradict.\n",
      "\n",
      "**Power Dynamics**\n",
      "In postcolonial contexts or regions with histories of medical exploitation, AI systems deployed by external entities may be perceived as continuing patterns of extraction and control rather than empowerment.\n",
      "\n",
      "## Mitigation Approaches\n",
      "\n",
      "Effective approaches would include community co-design of AI systems, cultural competency requirements in AI development, and governance frameworks that prioritize local autonomy while benefiting from AI capabilities.\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises significant ethical concerns. Here are some potential implications and considerations that may vary across different cultural contexts:\n",
      "\n",
      "**Ethical concerns:**\n",
      "\n",
      "1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory patterns if they are trained on biased data or designed with a discriminatory goal. This can lead to unequal access to care and healthcare outcomes for marginalized communities.\n",
      "2. **Lack of transparency and accountability**: Complex AI decision-making processes may be difficult to understand, making it challenging to identify and address errors or biases that could harm individuals or communities.\n",
      "3. **Algorithmic injustice**: Certain AI systems may disproportionately affect marginalized populations, perpetuating existing health disparities.\n",
      "4. **Informed consent**: Individuals within marginalized communities may not be fully informed about how their data is being used or by which entities, raising concerns about autonomy and trust in the decision-making process.\n",
      "\n",
      "**Variations across cultural contexts:**\n",
      "\n",
      "1. **Cultural values and norms**: Different cultures place varying emphasis on community, family ties, or individualism, and these may influence the design of AI systems and how they are implemented.\n",
      "2. **Language barriers**: Communication between service providers and patients from diverse linguistic backgrounds can create barriers to accessing healthcare services and make it more challenging to resolve errors or disputes.\n",
      "3. **Power dynamics**: AI decision-making processes may be influenced by power imbalances within communities, such as between healthcare providers and patients with varying levels of education or socio-economic status.\n",
      "4. **Local customs and practices**: In some cultures, there may be unique customs and practices related to health care delivery, which AI systems should respect and adapt to ensure effective collaboration.\n",
      "\n",
      "**Mitigating strategies:**\n",
      "\n",
      "1. **Interdisciplinary collaboration**: Multidisciplinary teams will be essential in addressing the complexities of AI decision-making processes in public health.\n",
      "2. **Cultural sensitivity training**: Providers should receive training on cultural differences and nuances related to marginalized populations.\n",
      "3. **Inclusive data collection**: Data sets used to train AI systems should reflect diverse populations and contexts, addressing biases and disparities.\n",
      "4. **Clear communication**: Patients and community members should be actively engaged in the decision-making process, ensuring that their concerns are heard and addressed.\n",
      "5. **Adaptability and iterative improvement**: AI systems will require continuous evaluation, adjustment, and refinement to address emerging issues and ensure fairness and equity.\n",
      "\n",
      "By acknowledging these complexities and embracing inclusive, culturally sensitive approaches, we can harness the potential of advanced AI in public health while minimizing harm to marginalized communities.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "The use of advanced AI systems in public health decision-making holds immense promise for improving efficiency, diagnostics, and resource allocation. However, deploying these systems, especially in marginalized communities, introduces complex ethical challenges that are further complicated by varying cultural contexts.\n",
      "\n",
      "Here are the potential ethical implications:\n",
      "\n",
      "### Ethical Implications for Public Health Decision-Making in Marginalized Communities:\n",
      "\n",
      "1.  **Algorithmic Bias and Amplification of Health Disparities:**\n",
      "    *   **Data Bias:** AI systems learn from historical data. If this data disproportionately represents certain groups, contains historical biases (e.g., medical racism, under-diagnosis of certain conditions in specific ethnic groups), or has gaps for marginalized communities, the AI will learn and perpetuate these biases. For instance, diagnostic AI trained primarily on data from lighter skin tones might perform poorly on darker skin.\n",
      "    *   **Outcome Bias:** AI models can reflect and even exacerbate existing socioeconomic and health inequities. An AI recommending resource allocation might, based on historical patterns, de-prioritize communities that have been historically underserved, leading to reduced access to care or lower quality interventions.\n",
      "    *   **Proxy Discrimination:** AI might use proxies (e.g., zip codes, education levels) that correlate with race, income, or other protected characteristics, leading to indirect discrimination, even if the system isn't explicitly programmed to do so.\n",
      "\n",
      "2.  **Privacy and Surveillance:**\n",
      "    *   **Excessive Data Collection:** AI systems require vast amounts of data, including sensitive health, genetic, and even behavioral information. Marginalized communities, often under greater scrutiny or with less digital literacy, might be disproportionately subject to data collection without full informed consent or understanding of its implications.\n",
      "    *   **Re-identification Risk:** Even \"anonymized\" data can sometimes be re-identified, especially with advanced AI techniques, posing risks to individuals and groups.\n",
      "    *   **Dual-Use Potential:** Public health data collected by AI could be misused for surveillance, immigration enforcement, or criminal justice purposes, eroding trust, particularly in communities with historical mistrust of authorities.\n",
      "\n",
      "3.  **Autonomy, Consent, and Paternalism:**\n",
      "    *   **Lack of Informed Consent:** Obtaining truly informed consent for complex AI interventions can be challenging. For marginalized communities, this is compounded by language barriers, lower literacy rates, power imbalances, and historical experiences of exploitation (e.g., Tuskegee Syphilis Study).\n",
      "    *   **Coercion/Pressure:** If AI systems become the primary decision-makers or gatekeepers for essential services, individuals might feel pressured to comply with AI recommendations even if they don't fully understand or agree, eroding their autonomy.\n",
      "    *   **Digital Divide:** Lack of access to technology, reliable internet, or digital literacy can exclude marginalized individuals from participating in or benefiting from AI-driven public health initiatives, further disempowering them.\n",
      "\n",
      "4.  **Accountability and Transparency (The \"Black Box\" Problem):**\n",
      "    *   **Lack of Explainability:** Many advanced AI systems (deep learning) are \"black boxes,\" meaning their decision-making processes are opaque. When an AI system makes a critical public health decision (e.g., recommending a lockdown, triaging patients), it's hard to understand *why* it made that choice.\n",
      "    *   **Diffusion of Responsibility:** If an AI makes a wrong or harmful decision, who is accountable? The developer? The public health official who deployed it? This becomes even more complex when the impact falls disproportionately on vulnerable groups.\n",
      "    *   **Erosion of Trust:** If public health decisions are made by inscrutable algorithms, it can foster mistrust, especially if those decisions appear to perpetuate existing injustices or discriminate against certain groups.\n",
      "\n",
      "5.  **Equity and Access:**\n",
      "    *   **Exacerbating Resource Inequality:** If AI systems are costly to develop and deploy, they might be first adopted by well-resourced health systems, potentially widening the gap between areas with advanced AI tools and those without.\n",
      "    *   **Focus on Quantifiable Metrics:** AI systems tend to optimize for quantifiable metrics. This might lead to overlooking crucial qualitative factors, social determinants of health, or community-specific needs that are harder to measure but vital for marginalized communities.\n",
      "\n",
      "### Variation Across Different Cultural Contexts:\n",
      "\n",
      "Cultural context profoundly shapes how these ethical implications manifest and how acceptable AI-driven public health interventions are perceived.\n",
      "\n",
      "1.  **Data Privacy & Ownership:**\n",
      "    *   **Individualistic vs. Collectivistic Cultures:** Western individualistic cultures often emphasize personal data privacy and consent. In many collectivistic cultures, data might be viewed more communally, and consent might involve family elders or community leaders rather than just the individual. However, this doesn't necessarily mean less concern about privacy; it's just framed differently.\n",
      "    *   **Trust in Institutions:** Cultures vary widely in their trust in government, medical institutions, and technology companies. Low trust (e.g., due to historical colonial abuses or authoritarian regimes) will make AI adoption in public health far more challenging, regardless of the AI's technical merit.\n",
      "    *   **Indigenous Data Sovereignty:** Many Indigenous communities assert inherent rights to control their data, including health data, reflecting unique cultural understandings of information, knowledge, and self-determination.\n",
      "\n",
      "2.  **Concept of Health & Well-being:**\n",
      "    *   **Biomedical vs. Holistic Views:** Many cultures have a holistic view of health, integrating physical, mental, spiritual, and community well-being. AI optimized for purely biomedical outcomes might miss or even contradict these broader cultural understandings, leading to ineffective or culturally inappropriate interventions.\n",
      "    *   **Traditional Medicine:** In cultures where traditional or indigenous medicine plays a significant role, AI-driven public health approaches might clash with or be seen as dismissive of established healing practices.\n",
      "\n",
      "3.  **Decision-Making and Autonomy:**\n",
      "    *   **Family/Community Involvement:** In many cultures, health decisions are not purely individual but involve family members, elders, or community leaders. AI systems designed for individual-centric decision-making might not integrate well into these processes.\n",
      "    *   **Fatalism/Determinism:** Some cultural beliefs might influence attitudes towards health interventions, potentially affecting willingness to engage with AI-driven preventative or diagnostic tools.\n",
      "\n",
      "4.  **Communication and Language:**\n",
      "    *   **Linguistic Diversity:** Public health interventions must be linguistically appropriate. AI systems need to be trained on diverse linguistic datasets, and their outputs must be comprehensible and culturally resonant.\n",
      "    *   **Non-Verbal Cues:** AI relies heavily on structured data. In many cultures, non-verbal cues, storytelling, and nuanced communication are crucial for understanding health concerns and building rapport, which AI currently struggles to interpret.\n",
      "\n",
      "5.  **Spirituality and Belief Systems:**\n",
      "    *   **Religious/Spiritual Influence:** Religious or spiritual beliefs can profoundly influence health behaviors, treatment choices, and attitudes towards technology. AI interventions that do not respectfully acknowledge or integrate these beliefs may face strong resistance.\n",
      "\n",
      "In conclusion, while advanced AI systems offer significant potential for public health, their deployment, especially in marginalized communities and across diverse cultural contexts, requires profound ethical consideration. A human-centered, culturally sensitive, and community-engaged approach is crucial to ensure these systems genuinely improve health equity rather than exacerbating existing disparities or eroding fundamental rights and values. This necessitates diverse development teams, robust regulatory frameworks, continuous auditing for bias, and meaningful participation from the communities served.\n",
      "\n",
      "# Response from competitor 5\n",
      "\n",
      "The use of advanced AI systems in decision-making processes related to public health, particularly in marginalized communities, raises several potential ethical implications. Here are some possible considerations:\n",
      "\n",
      "1. **Bias and discrimination**: AI systems can perpetuate existing biases and discriminatory practices if they are trained on biased data or designed with a particular worldview. For example, an AI system that categorizes individuals based on their socioeconomic status or ethnicity may unfairly penalize or benefit marginalized communities.\n",
      "2. **Lack of transparency and accountability**: Complex AI decision-making processes can be opaque, making it difficult to understand how decisions were made. This lack of transparency can erode trust in the system and create opportunities for abuse.\n",
      "3. **Extrication bias**: AI systems may be trained on datasets that reflect existing power dynamics, which can lead to \"inferred\" or unintentionally biased decision-making processes.\n",
      "4. **Limited contextual understanding**: AI systems may not fully understand the nuances of human behavior, culture, and contexts, leading to decisions that are not tailored to specific populations.\n",
      "\n",
      "Ethical implications across different cultural contexts:\n",
      "\n",
      "1. **Diversity and inclusion**:\n",
      "In some cultures, AI systems may be seen as more trustworthy or capable than those developed in other countries due to factors like language proficiency or cultural norms.\n",
      "2. **Contextual understanding**: The extent of contextual awareness varies across cultures, and AI systems designed for one cultural context may not perform well in others.\n",
      "3. **Power dynamics**: In some cultures, there may be existing power imbalances that influence how AI systems are developed, deployed, or controlled.\n",
      "4. **Linguistic and social nuances**: Cultural differences in communication styles, idioms, and social norms can affect the way people understand and interpret AI-generated outputs.\n",
      "\n",
      "Examples of cultural variations:\n",
      "\n",
      "1. In some South Asian cultures, there may be a tendency to use AI-powered systems to filter out perceived \"undesirables,\" such as individuals with disabilities or marginalized groups.\n",
      "2. In some Indigenous communities, AI systems may be used to track land use patterns and monitor environmental changes, but these efforts may be framed in ways that prioritize European colonialism or resource extraction.\n",
      "3. In Arab cultures, social media is highly influential in shaping public opinion, which can be leveraged by policymakers to influence AI decision-making processes.\n",
      "\n",
      "Varying implications across different cultural contexts:\n",
      "\n",
      "1. **More severe biases**: AI system design and training may exacerbate existing biases in more culturally diverse societies, perpetuating systemic inequalities.\n",
      "2. **More complex solutions**: In contrast, cultural nuances might lead to more nuanced and context-specific AI solutions that account for power dynamics, language use, and social norms.\n",
      "\n",
      "To mitigate these concerns, it is essential to:\n",
      "\n",
      "1. **Conduct inclusive research and testing** across various cultural contexts to ensure AI systems are developed with a deep understanding of diverse experiences.\n",
      "2. **Engage in iterative, community-driven processes** where stakeholders collaborate on designing, deploying, and monitoring AI systems that respect cultural differences and values.\n",
      "3. **Implement robust audit trails**, data protection policies, and transparent reporting mechanisms to maintain trust and accountability.\n",
      "\n",
      "Ultimately, effective AI governance requires a multi-faceted approach that balances technological innovation with social responsibility, inclusivity, and responsiveness to diverse contexts and cultures.\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"4\", \"1\", \"2\", \"3\", \"5\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-2.5-flash-preview-05-20\n",
      "Rank 2: gpt-4o-mini\n",
      "Rank 3: claude-3-7-sonnet-latest\n",
      "Rank 4: llama3.2:1b\n",
      "Rank 5: llama3.2:1b\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Exercise</h2>\n",
    "            <span style=\"color:#00bfff;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/commercial.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            and common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
