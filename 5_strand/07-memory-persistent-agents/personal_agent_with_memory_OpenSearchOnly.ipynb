{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8797de2a",
   "metadata": {},
   "source": [
    "# Persisting memory across Strands Agents sessions\n",
    "\n",
    "In this example you will learn how to persist memory across different sessions in your Strands Agents. \n",
    "\n",
    "We will use the use case of an agent that does web search using a `duckduckgo` search API.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Explore the capabilities of a memory-powered Strands agent.\n",
    "- Learn how to store, retrieve, and list memories.\n",
    "- Understand how to perform web searches via the agent.\n",
    "- Interact with the agent in an interactive loop.\n",
    "\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "Storing memories:\n",
    "```\n",
    "Remember that I prefer tea over coffee\n",
    "```\n",
    "\n",
    "Retrieving memories:\n",
    "```\n",
    "What do I prefer to drink?\n",
    "```\n",
    "\n",
    "Listing all memories:\n",
    "```\n",
    "Show me everything you remember about me\n",
    "```\n",
    "\n",
    "### Tips for Memory Usage\n",
    "\n",
    "- Be explicit when asking the agent to remember information\n",
    "- Use specific queries to retrieve relevant memories\n",
    "- Memory persistence enables more natural and contextual conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468364c",
   "metadata": {},
   "source": [
    "## Setup and prerequisites\n",
    "\n",
    "### Prerequisites\n",
    "* Python 3.10+\n",
    "* AWS account and AWS credentials configured in the environment\n",
    "* Anthropic Claude 3.7 enabled on Amazon Bedrock\n",
    "* IAM role with permissions to create Amazon Bedrock Knowledge Base, Amazon S3 bucket and Amazon DynamoDB\n",
    "\n",
    "Let's now install the requirement packages for our Strands Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae029383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c92217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import Required Libraries\n",
    "import os\n",
    "import boto3\n",
    "from strands import Agent, tool\n",
    "from strands.models import bedrock\n",
    "from strands_tools import mem0_memory\n",
    "\n",
    "from ddgs import DDGS\n",
    "from ddgs.exceptions import DDGSException, RatelimitException\n",
    "\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bedrock.DEFAULT_BEDROCK_MODEL_ID = \"apac.anthropic.claude-3-7-sonnet-20250219-v1:0\" #Optional: Set a default model for Bedrock\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2') # Initialize embedding model for vector operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b45276",
   "metadata": {},
   "source": [
    "## Alternative: Direct OpenSearch Implementation\n",
    "\n",
    "For users who prefer to use OpenSearch directly without the Mem0 abstraction layer, we'll also show how to create custom OpenSearch tools. This gives you more control over the vector storage and retrieval operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2bbe4-0a89-4b15-93f3-4baa9fc804c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR - Run the script to Create Opensearch Serverless resource in your AWS Account\n",
    "#!sh prereqs/deploy_OSS.sh Mac Users\n",
    "! \"E:\\Program Files\\Git\\bin\\bash.exe\" -c prereqs/deploy_OSS.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468a998-ebe9-4277-913f-6ea69de8e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can manually define your Opensearch Host \n",
    "os.environ[\"OPENSEARCH_HOST\"] = \"please enter your open search host\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b3163-cd88-4054-969c-0075dfc1dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Opensearch Serverless\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # take Opensearch environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenSearch client\n",
    "def get_opensearch_client():\n",
    "    \"\"\"Initialize OpenSearch client with AWS authentication\"\"\"\n",
    "    host = os.getenv('OPENSEARCH_HOST')\n",
    "    region = os.getenv('AWS_REGION', 'ap-southeast-2')\n",
    "    \n",
    "    if not host:\n",
    "        raise ValueError(\"OPENSEARCH_HOST environment variable not set\")\n",
    "    \n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    auth = AWSV4SignerAuth(credentials, region, 'aoss')\n",
    "    \n",
    "    client = OpenSearch(\n",
    "        hosts=[{'host': host, 'port': 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        pool_maxsize=20,\n",
    "    )\n",
    "    return client\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "def ensure_index_exists(client, index_name=\"memory-index\"):\n",
    "    \"\"\"Create the memory index if it doesn't exist\"\"\"\n",
    "    if not client.indices.exists(index=index_name):  # Add index= keyword\n",
    "        index_body = {\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"knn\": True,\n",
    "                    \"knn.algo_param.ef_search\": 100\n",
    "                }\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\"},\n",
    "                    \"user_id\": {\"type\": \"keyword\"},\n",
    "                    \"timestamp\": {\"type\": \"date\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": 384  # all-MiniLM-L6-v2 embedding dimension\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        client.indices.create(index=index_name, body=index_body)  # Add index= keyword\n",
    "        print(f\"Created index: {index_name}\")\n",
    "    else:\n",
    "        print(f\"Index {index_name} already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b46c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom OpenSearch memory tools\n",
    "@tool\n",
    "def opensearch_memory_store(content: str, user_id: str, index_name: str = \"memory-index\") -> str:\n",
    "    \"\"\"Store information in OpenSearch for later retrieval.\n",
    "    \n",
    "    Args:\n",
    "        content (str): The information to store\n",
    "        user_id (str): Unique identifier for the user\n",
    "        index_name (str): OpenSearch index name (default: memory-index)\n",
    "        \n",
    "    Returns:\n",
    "        str: Success or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = get_opensearch_client()\n",
    "        ensure_index_exists(client, index_name)\n",
    "        \n",
    "        # Generate embedding for the content\n",
    "        embedding = embedding_model.encode(content).tolist()\n",
    "        \n",
    "        # Create document\n",
    "        doc = {\n",
    "            \"content\": content,\n",
    "            \"user_id\": user_id,\n",
    "            #\"timestamp\": \"now\",\n",
    "            \"embedding\": embedding\n",
    "        }\n",
    "        \n",
    "        # Store document\n",
    "        #doc_id = str(uuid.uuid4())\n",
    "        response = client.index(\n",
    "            index=index_name,\n",
    "            #id=doc_id,\n",
    "            body=doc\n",
    "        )\n",
    "        \n",
    "        return f\"Successfully stored memory with ID: {response}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error storing memory: {str(e)}\"\n",
    "\n",
    "@tool \n",
    "def opensearch_memory_retrieve(query: str, user_id: str, index_name: str = \"memory-index\", top_k: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant memories from OpenSearch based on a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query to search for\n",
    "        user_id (str): Unique identifier for the user\n",
    "        index_name (str): OpenSearch index name (default: memory-index)\n",
    "        top_k (int): Number of top results to return (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        str: Retrieved memories or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = get_opensearch_client()\n",
    "        \n",
    "        # Generate embedding for the query\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "        \n",
    "        # Search for similar memories\n",
    "        search_body = {\n",
    "            \"size\": top_k,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"term\": {\"user_id\": user_id}}\n",
    "                    ],\n",
    "                    \"should\": [\n",
    "                        {\n",
    "                            \"knn\": {\n",
    "                                \"embedding\": {\n",
    "                                    \"vector\": query_embedding,\n",
    "                                    \"k\": top_k\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = client.search(index=index_name, body=search_body)\n",
    "        \n",
    "        if response['hits']['total']['value'] == 0:\n",
    "            return \"No memories found for this query.\"\n",
    "        \n",
    "        memories = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            score = hit['_score']\n",
    "            content = hit['_source']['content']\n",
    "            memories.append(f\"Score: {score:.2f} - {content}\")\n",
    "        \n",
    "        return \"Retrieved memories:\\\\n\" + \"\\\\n\".join(memories)\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving memories: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def opensearch_memory_list(user_id: str, index_name: str = \"memory-index\", limit: int = 10) -> str:\n",
    "    \"\"\"List all stored memories for a user.\n",
    "    \n",
    "    Args:\n",
    "        user_id (str): Unique identifier for the user\n",
    "        index_name (str): OpenSearch index name (default: memory-index)\n",
    "        limit (int): Maximum number of memories to return (default: 10)\n",
    "        \n",
    "    Returns:\n",
    "        str: List of all memories or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = get_opensearch_client()\n",
    "        \n",
    "        search_body = {\n",
    "            \"size\": limit,\n",
    "            \"query\": {\n",
    "                \"term\": {\"user_id\": user_id}\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        response = client.search(index=index_name, body=search_body)\n",
    "        \n",
    "        if response['hits']['total']['value'] == 0:\n",
    "            return \"No memories found for this user.\"\n",
    "        \n",
    "        memories = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            content = hit['_source']['content']\n",
    "            #timestamp = hit['_source']['timestamp']\n",
    "            memories.append(f\"{content}\")\n",
    "        \n",
    "        return f\"All stored memories ({response['hits']['total']['value']} total):\\\\n\" + \"\\\\n\".join(memories)\n",
    "    except Exception as e:\n",
    "        return f\"Error listing memories: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679ce42",
   "metadata": {},
   "source": [
    "## Define System Prompt\n",
    "\n",
    "The `SYSTEM_PROMPT` variable defines the behavior and capabilities of the memory agent. This prompt guides the agent to provide personalized responses based on stored memories and perform web searches when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01951bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a focused system prompt for memory operations\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful personal assistant for a user. Your task is to assist the user by providing personalized responses based on their history. \n",
    "\n",
    "Capabilities:\n",
    "- You can store information using the mem0_memory tool (action=\"store\").\n",
    "- You can retrieve relevant memories using the mem0_memory tool (action=\"retrieve\").\n",
    "- You can use duckduckgo_search to find information on the web.\n",
    "\n",
    "Key Rules:\n",
    "- Be conversational and natural in your responses.\n",
    "- Always retrieve memories before responding to the user and use them to inform your response.\n",
    "- Store any new user information and user preferences in mem0_memory.\n",
    "- Only share relevant information.\n",
    "- Politely indicate when you don't have the information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a7192",
   "metadata": {},
   "source": [
    "## Define Web Search Tool\n",
    "\n",
    "The `websearch` tool using [Duckduckgo Search API](https://github.com/deedy5/duckduckgo_search) function allows the agent to perform web searches. This function handles exceptions and returns search results or appropriate error messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5d26e",
   "metadata": {},
   "source": [
    "## Alternative: Create Memory Agent with Direct OpenSearch\n",
    "\n",
    "Here's an alternative implementation that uses direct OpenSearch operations instead of the Mem0 abstraction layer. This approach gives you more control over the vector storage and retrieval process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def websearch(\n",
    "    keywords: str,\n",
    "    region: str = \"us-en\",\n",
    "    max_results: int | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Search the web to get updated information.\n",
    "    Args:\n",
    "        keywords (str): The search query keywords.\n",
    "        region (str): The search region: wt-wt, us-en, uk-en, ru-ru, etc..\n",
    "        max_results (int | None): The maximum number of results to return.\n",
    "    Returns:\n",
    "        List of dictionaries with search results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = DDGS().text(keywords, region=region, max_results=max_results)\n",
    "        return results if results else \"No results found.\"\n",
    "    except RatelimitException:\n",
    "        return \"RatelimitException: Please try again after a short delay.\"\n",
    "    except DDGSException as d:\n",
    "        return f\"DuckDuckGoSearchException: {d}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7828a",
   "metadata": {},
   "source": [
    "## Create Memory Agent\n",
    "\n",
    "We will now initialize the memory-focused agent using the defined tools and system prompt. The Strands agent is capable of:\n",
    "1. Storing and retrieving memories based on context. It uses memory to create more personalized and contextual AI interactions.\n",
    "2. Performing web searches using DuckDuckGo to give updated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with direct OpenSearch memory tools\n",
    "USER_ID = \"PeterRabbit\"\n",
    "opensearch_memory_agent = Agent(\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    model=\"apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",  # Optional: Specify the model ID\n",
    "    tools=[opensearch_memory_store, opensearch_memory_retrieve, opensearch_memory_list, websearch],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae37ce",
   "metadata": {},
   "source": [
    "## Alternative: Demonstrate OpenSearch Memory Operations\n",
    "\n",
    "The following examples demonstrate how to store, retrieve, and list memories using the direct OpenSearch implementation.\n",
    "\n",
    "- **opensearch_memory_store**: Save important information directly to OpenSearch\n",
    "- **opensearch_memory_retrieve**: Access relevant memories based on semantic similarity\n",
    "- **opensearch_memory_list**: View all stored memories for a user\n",
    "\n",
    "This approach provides more control over vector embeddings and search parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial memories using direct OpenSearch tools\n",
    "print(\"Storing memories using OpenSearch...\")\n",
    "\n",
    "# Store user information\n",
    "result1 = opensearch_memory_store(\n",
    "    f\"The user's name is {USER_ID}.\", USER_ID)\n",
    "\n",
    "print(\"Store result 1:\", result1)\n",
    "\n",
    "# Store user preferences  \n",
    "result2 = opensearch_memory_store(\"I like to drink tea more than coffee.\", USER_ID)\n",
    "print(\"Store result 2:\", result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve memories using semantic search\n",
    "print(\"Retrieving memories using OpenSearch...\")\n",
    "\n",
    "# Retrieve information about user's name\n",
    "retrieved_name = opensearch_memory_retrieve(\"What is the user's name?\", USER_ID)\n",
    "print(\"Retrieved name info:\", retrieved_name)\n",
    "\n",
    "# Retrieve information about user's preferences\n",
    "retrieved_prefs = opensearch_memory_retrieve(\"What are the user's drink preferences?\", USER_ID)\n",
    "print(\"Retrieved preferences:\", retrieved_prefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the agent a question using OpenSearch memory\n",
    "response = opensearch_memory_agent(\"What are the events happening in New York today?\")\n",
    "print(\"Agent response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5472374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all stored memories using OpenSearch\n",
    "print(\"Listing all stored memories...\")\n",
    "all_memories = opensearch_memory_list(USER_ID)\n",
    "print(\"All memories:\", all_memories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c72c91",
   "metadata": {},
   "source": [
    "## Interactive Agent Usage\n",
    "\n",
    "Finally, we provide an interactive loop for users to interact with the memory agent. Users can store new memories, retrieve existing ones, or list all stored memories.\n",
    "\n",
    "To test interactive usage: \n",
    "1. Install the requirements: `pip install -r requirements.txt`\n",
    "1. Run the python file `personal_agent_with_memory.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ea59c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to create a personal agent with memory capabilities using the Strands framework. The agent can:\n",
    "\n",
    "1. Store information about the user\n",
    "2. Retrieve relevant memories based on context\n",
    "3. Search the web for additional information\n",
    "4. Provide personalized responses\n",
    "\n",
    "By combining these capabilities, the agent can maintain context across conversations and provide more personalized assistance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fd03a-26cc-4190-bb09-872c6767f972",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Run this bash script to clean up the Opensearch Serverless resources. You don't need to run this if you used the \"MEM0_PLATFORM_API\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6113d4f-45d2-46a5-9853-335b26da0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sh prereqs/cleanup_OSS.sh #Mac Users\n",
    "! \"E:\\Program Files\\Git\\bin\\bash.exe\" -c prereqs/cleanup_OSS.sh "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentic AI Education",
   "language": "python",
   "name": "agentic_ai_education"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
